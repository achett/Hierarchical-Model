{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achett/Hierarchical-Model/blob/main/Bayesian_Hierarchical_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WPJgqdPdNT1D",
        "outputId": "05b55bc7-4569-4f24-9148-0c1cdfa0dd19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hierarchicalforecast\n",
            "  Downloading hierarchicalforecast-0.4.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.24 (from hierarchicalforecast)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (0.58.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (1.2.2)\n",
            "Collecting quadprog (from hierarchicalforecast)\n",
            "  Downloading quadprog-0.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.2/508.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->hierarchicalforecast) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hierarchicalforecast) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->hierarchicalforecast) (1.16.0)\n",
            "Installing collected packages: numpy, quadprog, hierarchicalforecast\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hierarchicalforecast-0.4.1 numpy-1.23.5 quadprog-0.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "dca3bf2e6b1b4ae8aacd127fcb36768d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting statsforecast\n",
            "  Downloading statsforecast-1.7.3-py3-none-any.whl (120 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/120.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m112.6/120.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from statsforecast) (2.2.1)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from statsforecast) (4.66.2)\n",
            "Collecting fugue>=0.8.1 (from statsforecast)\n",
            "  Downloading fugue-0.8.7-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast>=0.0.24 (from statsforecast)\n",
            "  Downloading utilsforecast-0.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from statsforecast) (3.3.0)\n",
            "Collecting triad>=0.9.3 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading triad-0.9.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting qpd>=0.4.4 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading qpd-0.4.4-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugue-sql-antlr>=0.1.6 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading fugue-sql-antlr-0.2.0.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (20.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (3.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.0->statsforecast) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2023.4)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (24.0)\n",
            "Collecting antlr4-python3-runtime<4.12 (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast)\n",
            "  Downloading antlr4_python3_runtime-4.11.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels>=0.13.2->statsforecast) (1.16.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast) (14.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast) (2023.6.0)\n",
            "Collecting fs (from triad>=0.9.3->fugue>=0.8.1->statsforecast)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->fugue>=0.8.1->statsforecast) (2.1.5)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast) (67.7.2)\n",
            "Building wheels for collected packages: fugue-sql-antlr\n",
            "  Building wheel for fugue-sql-antlr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fugue-sql-antlr: filename=fugue_sql_antlr-0.2.0-py3-none-any.whl size=158196 sha256=a6d2dfe835c2139ac39631f3617825de5ab6dac494a9bd068bf5e68d5c9941b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/b5/4e/216953a1c711da55de29ed7ecf158b4a5bf32ef93d69ad66dd\n",
            "Successfully built fugue-sql-antlr\n",
            "Installing collected packages: antlr4-python3-runtime, fs, utilsforecast, triad, fugue-sql-antlr, adagio, qpd, fugue, statsforecast\n",
            "Successfully installed adagio-0.2.4 antlr4-python3-runtime-4.11.1 fs-2.4.16 fugue-0.8.7 fugue-sql-antlr-0.2.0 qpd-0.4.4 statsforecast-1.7.3 triad-0.9.5 utilsforecast-0.1.1\n",
            "Collecting datasetsforecast\n",
            "  Downloading datasetsforecast-0.0.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (3.9.3)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (0.8.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (4.66.2)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.0.1)\n",
            "Requirement already satisfied: triad>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.9.5)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.2.0)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (20.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (3.1.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (4.0.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->datasetsforecast) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasetsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasetsforecast) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2024.2.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->datasetsforecast) (4.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->datasetsforecast) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasetsforecast) (1.16.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (14.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (2023.6.0)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (2.4.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->fugue>=0.8.1->datasetsforecast) (2.1.5)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (67.7.2)\n",
            "Installing collected packages: datasetsforecast\n",
            "Successfully installed datasetsforecast-0.0.8\n",
            "Collecting darts\n",
            "  Downloading darts-0.28.0-py3-none-any.whl (846 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.9/846.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from darts) (0.44)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from darts) (3.7.1)\n",
            "Collecting nfoursid>=1.0.0 (from darts)\n",
            "  Downloading nfoursid-1.0.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.23.5)\n",
            "Collecting pmdarima>=1.8.0 (from darts)\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyod>=0.9.5 (from darts)\n",
            "  Downloading pyod-1.1.3.tar.gz (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from darts) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from darts) (1.11.4)\n",
            "Collecting shap>=0.40.0 (from darts)\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsforecast>=1.4 in /usr/local/lib/python3.10/dist-packages (from darts) (1.7.3)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from darts) (0.14.1)\n",
            "Collecting tbats>=1.1.0 (from darts)\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.10/dist-packages (from darts) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from darts) (4.10.0)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2023.7.0)\n",
            "Requirement already satisfied: xgboost>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.3)\n",
            "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=2.1 (from darts)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from darts) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->darts) (2023.4)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (3.0.9)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (67.7.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from pyod>=0.9.5->darts) (0.58.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.9.5->darts) (1.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.8.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->darts) (3.3.0)\n",
            "Collecting slicer==0.0.7 (from shap>=0.40.0->darts)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.40.0->darts) (2.2.1)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.8.7)\n",
            "Requirement already satisfied: utilsforecast>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.0->darts) (0.5.6)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.1->darts) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->darts)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.9.3)\n",
            "Requirement already satisfied: triad>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.9.5)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.0)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (20.11.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->darts) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->darts) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (4.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=1.4->darts) (4.11.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (14.0.2)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (2.4.16)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.1.3-py3-none-any.whl size=190250 sha256=8b7921e47f80996fe0b7eedf5ae01cdbab4504d1971e77c478e920148197ed13\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/f8/db/124d43bec122d6ec0ab3713fadfe25ebed8af52ec561682b4e\n",
            "Successfully built pyod\n",
            "Installing collected packages: tensorboardX, slicer, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, shap, pyod, nvidia-cusolver-cu12, nfoursid, pmdarima, torchmetrics, tbats, pytorch-lightning, darts\n",
            "Successfully installed darts-0.28.0 lightning-utilities-0.10.1 nfoursid-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pmdarima-2.0.4 pyod-1.1.3 pytorch-lightning-2.2.1 shap-0.45.0 slicer-0.0.7 tbats-1.1.3 tensorboardX-2.6.2.2 torchmetrics-1.3.1\n",
            "Collecting mlforecast\n",
            "  Downloading mlforecast-0.12.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlforecast) (2.2.1)\n",
            "Collecting coreforecast>=0.0.7 (from mlforecast)\n",
            "  Downloading coreforecast-0.0.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.5/193.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from mlforecast) (2023.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast) (0.58.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlforecast) (24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mlforecast) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mlforecast) (1.2.2)\n",
            "Requirement already satisfied: utilsforecast>=0.0.27 in /usr/local/lib/python3.10/dist-packages (from mlforecast) (0.1.1)\n",
            "Collecting window-ops (from mlforecast)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from coreforecast>=0.0.7->mlforecast) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mlforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mlforecast) (2023.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast) (0.41.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->mlforecast) (1.16.0)\n",
            "Installing collected packages: coreforecast, window-ops, mlforecast\n",
            "Successfully installed coreforecast-0.0.7 mlforecast-0.12.0 window-ops-0.0.15\n"
          ]
        }
      ],
      "source": [
        "!pip install hierarchicalforecast\n",
        "!pip install statsforecast\n",
        "!pip install datasetsforecast\n",
        "!pip install nixtlats>=0.1.0\n",
        "!pip install darts\n",
        "!pip install mlforecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uUqzb8wbrU_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f02a6c-6e64-41f8-d511-42e6894e3607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "# PACKAGES\n",
        "########################\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import openpyxl\n",
        "from datetime import datetime\n",
        "\n",
        "from statsforecast.core import StatsForecast\n",
        "from statsforecast.models import AutoARIMA, Naive, AutoETS, AutoCES, AutoTheta\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from hierarchicalforecast.core import HierarchicalReconciliation\n",
        "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
        "from hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut, MinTrace, OptimalCombination, ERM, PERMBU, Bootstrap, Normality\n",
        "from hierarchicalforecast.utils import aggregate\n",
        "from nixtlats import TimeGPT\n",
        "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
        "\n",
        "\n",
        "from darts import TimeSeries, concatenate\n",
        "from darts.models import RegressionModel, LightGBMModel, ExponentialSmoothing, StatsForecastAutoETS, StatsForecastAutoARIMA, KalmanForecaster\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from lightgbm import LGBMRegressor\n",
        "from darts.metrics import mae, rmse, mape, quantile_loss, mse, ope\n",
        "from darts.utils.likelihood_models import QuantileRegression\n",
        "\n",
        "pd.options.display.float_format = '{:,.2f}'.format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6iyKhHrjGb0t"
      },
      "outputs": [],
      "source": [
        "##############\n",
        "# PARAMS\n",
        "##############\n",
        "fct_periods = 12\n",
        "fct_st_date = '2023-04-01'\n",
        "fct_end_date = '2023-12-01'\n",
        "\n",
        "# Create hierarchical structure and constraints\n",
        "hierarchy_levels = [['TopLv'],\n",
        "                    ['TopLv', 'ProductLv'],\n",
        "                    ['TopLv', 'ProductLv', 'Lv1'],\n",
        "                    ['TopLv', 'ProductLv', 'Lv1', 'Lv2'],\n",
        "                    ['TopLv', 'ProductLv', 'Lv1', 'Lv2', 'Lv3'],\n",
        "                    ['TopLv', 'ProductLv', 'Lv1', 'Lv2', 'Lv3', 'Lv4'],\n",
        "                    ['TopLv', 'ProductLv', 'Lv1', 'Lv2', 'Lv3', 'Lv4', 'Lv5']]\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/regional_hierarchy.xlsx'\n",
        "sheet_name = 'regional_hierarchy v2'\n",
        "r_hier = pd.read_excel(inputFile, sheet_name=sheet_name)\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/model_selection.xlsx'\n",
        "model_selection = pd.read_excel(inputFile)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "# FUNCTIONS\n",
        "##############\n",
        "def prepare_data(data, r_hier):\n",
        "    # Merge hierarchy\n",
        "    data = data.merge(r_hier, how='inner', left_on='cost_object', right_on='Lv5')\n",
        "\n",
        "    # Transform date and y\n",
        "    data['ds'] = pd.to_datetime(data['ds'])\n",
        "    data['y'] = data['y'].astype(float)\n",
        "\n",
        "    # Address NA values\n",
        "    data['y'] = data['y'].fillna(0)\n",
        "    data['TopLv'] = data['TopLv'].fillna('')\n",
        "    data['Lv1'] = data['Lv1'].fillna('')\n",
        "    data['Lv2'] = data['Lv2'].fillna('')\n",
        "    data['Lv3'] = data['Lv3'].fillna('')\n",
        "    data['Lv4'] = data['Lv4'].fillna('')\n",
        "    data['Lv5'] = data['Lv5'].fillna('')\n",
        "    data['product'] = data['product'].fillna('')\n",
        "\n",
        "    # Create hierarchical dataframe\n",
        "    data.rename(columns={'product': 'ProductLv'}, inplace=True)\n",
        "    data = data[['TopLv', 'ProductLv', 'Lv1', 'Lv2', 'Lv3', 'Lv4', 'Lv5', 'ds', 'y']]\n",
        "\n",
        "    # Replace '/' with '_' in the four columns\n",
        "    data['TopLv'] = data['TopLv'].str.replace('/', '_')\n",
        "    data['ProductLv'] = data['ProductLv'].str.replace('/', '_')\n",
        "    data['Lv1'] = data['Lv1'].str.replace('/', '_')\n",
        "    data['Lv2'] = data['Lv2'].str.replace('/', '_')\n",
        "    data['Lv3'] = data['Lv3'].str.replace('/', '_')\n",
        "    data['Lv4'] = data['Lv4'].str.replace('/', '_')\n",
        "    data['Lv5'] = data['Lv5'].str.replace('/', '_')\n",
        "\n",
        "    data['unique_id'] = data['TopLv'] + '/' + data['ProductLv'] + '/' + data['Lv1'] + '/' + data['Lv2'] + '/' + data['Lv3'] + '/' + data['Lv4'] + '/' + data['Lv5']\n",
        "\n",
        "    return data\n",
        "\n",
        "def prepare_feature(data, r_hier, volume_act2, feature_name):\n",
        "\n",
        "    # Select and rename columns\n",
        "    data = data[['cost_object', 'product', 'ds', feature_name]].rename(columns={feature_name: 'y'})\n",
        "\n",
        "    # Apply any additional preparation (assuming prepare_data is a function you have defined)\n",
        "    data = prepare_data(data, r_hier)\n",
        "\n",
        "    # Rename the columns back\n",
        "    data = data.rename(columns={'y': feature_name})\n",
        "\n",
        "    # Merge with the volume_act2 dataframe\n",
        "    merged_df = data.merge(volume_act2[['unique_id', 'ds']], how='right', on=['unique_id', 'ds'])\n",
        "\n",
        "    return merged_df\n"
      ],
      "metadata": {
        "id": "-0tzZ-xnIBTt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "# DATA LOAD\n",
        "##############\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/budgetFY23.csv'\n",
        "budget = pd.read_csv(inputFile)\n",
        "# budget = budget[budget['category']=='EQUIV_UNIT - Equivalent Units']\n",
        "budget = budget[budget['category']=='UC110000 - Total Revenue']\n",
        "budget.rename(columns={'country': 'cost_object'}, inplace=True)\n",
        "budget = prepare_data(budget, r_hier)\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/revenue_output.csv'\n",
        "volume_act = pd.read_csv(inputFile)\n",
        "volume_act.rename(columns={'value': 'y'}, inplace=True)\n",
        "volume_act = prepare_data(volume_act, r_hier)\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/SGA Prediction/data/sga_output.csv'\n",
        "sga = pd.read_csv(inputFile)\n",
        "\n",
        "sga1 = prepare_feature(sga, r_hier, volume_act, 'AP')\n",
        "sga2 = prepare_feature(sga, r_hier, volume_act, 'Field_Sales')"
      ],
      "metadata": {
        "id": "u3E-oM38IFVP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# SAMPLE\n",
        "########################\n",
        "# # Subset\n",
        "# regs2include = ['Global/ENZA - Enzalutamide/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/US10 - Astellas Pharma US, Inc.',  'Global/REGADENOSN - Regadenoson/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/US10 - Astellas Pharma US, Inc.', 'Global/ENZA - Enzalutamide/D_GCN - Greater China/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total']\n",
        "# volume_act = volume_act[volume_act['unique_id'].isin(regs2include)]\n",
        "\n",
        "# level1include = ['D_JPCOM - Japan Commercial']\n",
        "# volume_act = volume_act[volume_act['Lv1'].isin(level1include)]"
      ],
      "metadata": {
        "id": "vvMW8Aj7jmRz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# IDENTIFY UNIVERSE\n",
        "########################\n",
        "tested_ts = set(budget['unique_id'].unique()).intersection(volume_act['unique_id'].unique())\n",
        "\n",
        "# Find unique IDs present in budget_h but not in rev\n",
        "unique_ids_in_budget_not_in_rev = set(budget['unique_id'].unique()).difference(volume_act['unique_id'].unique())\n",
        "\n",
        "# Find unique IDs present in rev but not in budget_h\n",
        "unique_ids_in_rev_not_in_budget = set(volume_act['unique_id'].unique()).difference(budget['unique_id'].unique())\n",
        "\n",
        "# Filter volume\n",
        "volume_act = volume_act[volume_act['unique_id'].isin(tested_ts)]"
      ],
      "metadata": {
        "id": "JTjLsBc6ATQQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# SEGMENT TIME SERIES\n",
        "########################\n",
        "new_products = ['ENFORTUMAB - Enforumab Vedotin', 'ROXADUSTNT - Roxadustant']\n",
        "loe_products = ['REGADENOSN - Regadenoson']\n",
        "div_products = ['MICAFUNGIN - Micafungin Sodium']\n",
        "\n",
        "new_ids = volume_act[volume_act['ProductLv'].isin(new_products)]['unique_id'].unique().tolist()\n",
        "loe_ids = volume_act[volume_act['ProductLv'].isin(loe_products)]['unique_id'].unique().tolist()\n",
        "divested_ids = volume_act[volume_act['ProductLv'].isin(div_products)]['unique_id'].unique().tolist()\n",
        "\n",
        "# IDs with A&P and Field Sales Spend\n",
        "grouped1 = sga1.groupby('unique_id')[['AP']].sum()\n",
        "grouped2 = sga2.groupby('unique_id')[['Field_Sales']].sum()\n",
        "spend_ids = set(grouped1[(grouped1['AP'] > 0)].index.tolist() + grouped2[(grouped2['Field_Sales'] > 0)].index.tolist())\n",
        "spend_ids = spend_ids.difference(new_ids + loe_ids + divested_ids)\n",
        "\n",
        "# IDs with no spend\n",
        "non_spend_ids = volume_act[~volume_act['unique_id'].isin(spend_ids)]['unique_id'].unique()\n",
        "\n",
        "# Model Selection\n",
        "arima_regions = model_selection[model_selection['model']=='ARIMA']['Lv3'].unique()\n",
        "ets_regions = model_selection[model_selection['model']=='ETS']['Lv3'].unique()\n",
        "# arima_ids = volume_act[(volume_act['level3'].isin(arima_regions)) & (~volume_act['unique_id'].isin(spend_ids))]['unique_id'].unique().tolist()\n",
        "# ets_ids = volume_act[(volume_act['level3'].isin(ets_regions)) & (~volume_act['unique_id'].isin(spend_ids))]['unique_id'].unique().tolist()\n",
        "\n",
        "arima_ids = volume_act[(volume_act['Lv3'].isin(arima_regions))]['unique_id'].unique().tolist()\n",
        "ets_ids = volume_act[(volume_act['Lv3'].isin(ets_regions))]['unique_id'].unique().tolist()\n",
        "\n",
        "# Solifenacin _ Tamsulosin\n",
        "solif_tams_ids = volume_act[volume_act['ProductLv'].isin(['SOLIF_TAMS - Solifenacin _ Tamsulosin', 'TAMSULOSIN - Tamsulosin HCl', 'TAMSUL_TAB - Tamsulosin tab'])]['unique_id'].unique().tolist()\n",
        "arima_ids = set(arima_ids+solif_tams_ids)\n",
        "ets_ids = [id for id in ets_ids if id not in solif_tams_ids]"
      ],
      "metadata": {
        "id": "Y7_vce08-o24"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# DATA CONVERSION\n",
        "########################\n",
        "set2zero_list=['Global/TAMSULOSIN - Tamsulosin HCl/D_GCN - Greater China/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total/D_CN_TOTAL - China Total',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BENELUX - Benelux/D_E_BELGIUM - Belgium/D_E_BELGIUM - Belgium',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BBMCI - BBMCI group/D_E_BALKANS - Balkans/D_E_BOS_HER - Bosnia-Herz.',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BBMCI - BBMCI group/D_E_BALKANS - Balkans/D_E_BOS_HER - Bosnia-Herz.',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ADRCS_PT - Adriatics & Portugal/D_E_ADRCS - Adriatic Adriatics/D_E_CROATIA - Croatia',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ADRCS_PT - Adriatics & Portugal/D_E_ADRCS - Adriatic Adriatics/D_E_CROATIA - Croatia',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_CZECH - Czech',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_HUBGROGR - HBRG/D_E_HUBGRO - Hungary  Bulgaria & Romania/D_E_HU - Hungary',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_IE - Ireland/D_E_IE - Ireland/D_E_IE - Ireland',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BBMCI - BBMCI group/D_E_MTCYIS - Malta  Cyprus & Iceland/D_E_MALTA - Malta',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_NORDIC - Nordic/D_E_NORWAY - Norway/D_E_NORWAY - Norway',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_PO - Poland/D_E_PO - Poland',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_PO - Poland/D_E_PO - Poland',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_SLOVAKIA - Slovakia',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_SPAIN - Spain/D_E_SPAIN - Spain/D_E_SPAIN - Spain/D_E_SPAIN - Spain',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_UA - Ukraine/D_E_UA - Ukraine',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_UA - Ukraine/D_E_UA - Ukraine',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_GCN - Greater China/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_GCN - Greater China/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total/D_HK_TOTAL - Hong Kong Total',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_BEL - Belarus/D_I_CIS_BEL - Belarus/D_I_CIS_BEL - Belarus',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_BEL - Belarus/D_I_CIS_BEL - Belarus/D_I_CIS_BEL - Belarus',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_EGYPT - Egypt/D_I_EGYPT - Egypt/D_I_EGYPT - Egypt',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_APAC_CORE - APAC CORE/D_I_INDONESIA - Indonesia/D_I_INDONESIA - Indonesia/D_I_INDONESIA - Indonesia',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_MEA_DB - Distributor Business/D_I_IRAQ - Iraq/D_I_IRAQ - Iraq/D_I_IRAQ - Iraq',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_MEA_DB - Distributor Business/D_I_JORDAN - Jordan/D_I_JORDAN - Jordan/D_I_JORDAN - Jordan',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_LATAM - Domestic - LatAM/D_I_LATAM_REST - Domestic Rest of Latam/D_I_LATAM_REST_OTH - Domestic Rest of Latam Others/D_I_LATAM_REST_OTH - Domestic Rest of Latam Others',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_MEA_DB - Distributor Business/D_I_LEBANON - Lebanon/D_I_LEBANON - Lebanon/D_I_LEBANON - Lebanon',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_APAC_CORE - APAC CORE/D_I_PHILIPPINES - Philippines/D_I_PHILIPPINES - Philippines/D_I_PHILIPPINES - Philippines',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_SAFRICA - South Africa/D_I_SAFRICA - South Africa/D_I_SAFRICA - South Africa',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_APAC_CORE - APAC CORE/D_I_SINMAL - SINMAL/D_I_SINGAPORE - SINGAPORE/D_I_SINGAPORE - SINGAPORE',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_I_INTL - International Markets/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_I_INTL - International Markets/D_I_APAC_CORE - APAC CORE/D_I_VIETNAM - Vietnam/D_I_VIETNAM - Vietnam/D_I_VIETNAM - Vietnam',\n",
        "       'Global/TAMSULOSIN - Tamsulosin HCl/D_GCN - Greater China/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total',\n",
        "       'Global/TAMSUL_TAB - Tamsulosin tab/D_GCN - Greater China/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total/D_TW_TOTAL - Taiwan Total',\n",
        "      'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_E_ESTMKT - Established Markets/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy',\n",
        "       'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BENELUX - Benelux/D_E_NETHLND - Netherlands/D_E_NETHLND - Netherlands',\n",
        "       'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_SLOVAKIA - Slovakia',\n",
        "       'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_UA - Ukraine/D_E_UA - Ukraine',\n",
        "       'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_I_INTL - International Markets/D_I_LATAM - Domestic - LatAM/D_I_LATAM_DB - Domestic Latam Distributor Business/D_I_AR - Domestic Argentina/D_I_AR - Domestic Argentina',\n",
        "       'Global/SOLIF_TAMS - Solifenacin _ Tamsulosin/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan/D_I_CIS_KAZ - Kazakhstan']\n",
        "\n",
        "volume_act.loc[(volume_act['unique_id'].isin(set2zero_list)) & (volume_act['ds'] < '2022-04-01'), 'y'] = 0"
      ],
      "metadata": {
        "id": "5scRcvJJl5fi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ########################\n",
        "# # INTERMITTENT DEMAND CANDIDATES\n",
        "# ########################\n",
        "\n",
        "# # Function to calculate the percentage of zeros after the first non-zero\n",
        "# def calculate_percentage_zeros(df):\n",
        "#     # Find the index of the first non-zero entry in 'y'\n",
        "#     first_non_zero_index = df.loc[df['y'] != 0].index.min()\n",
        "#     # If there are no non-zero values, return None or 0 based on your preference\n",
        "#     if pd.isna(first_non_zero_index):\n",
        "#         return None  # Or return 0 if you want to treat this as 0% zeros following non-zero\n",
        "#     # Select the subset of 'y' after the first non-zero\n",
        "#     post_non_zero_series = df.loc[first_non_zero_index:, 'y']\n",
        "#     # Count the number of zeros in this subset\n",
        "#     num_zeros = (post_non_zero_series == 0).sum()\n",
        "#     # Calculate the percentage of zeros\n",
        "#     percentage_zeros = num_zeros / len(post_non_zero_series) * 100\n",
        "#     return percentage_zeros\n",
        "\n",
        "# # Apply the function to each group and reset index to make unique_id a column\n",
        "# percentage_zeros_df = volume_act.groupby('unique_id').apply(calculate_percentage_zeros).reset_index(name='percentage_zeros')\n",
        "\n",
        "# inter_demand_ids = percentage_zeros_df[percentage_zeros_df['percentage_zeros']>=50]['unique_id'].tolist()\n"
      ],
      "metadata": {
        "id": "Aw-07I-33MIr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RUN ETS & ARIMA\n",
        "########################\n",
        "def convert_fct2df(forecasts):\n",
        "    forecast_dfs = []\n",
        "    for unique_id, forecast_ts in forecasts.items():\n",
        "        df = TimeSeries.quantiles_df(forecast_ts, quantiles=[0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995])\n",
        "        df['unique_id'] = unique_id\n",
        "        df = df.reset_index()\n",
        "        df = df.rename(columns={'y_0.5': 'y'})\n",
        "        forecast_dfs.append(df)\n",
        "\n",
        "    # Concatenate all forecast DataFrames into a single DataFrame\n",
        "    all_forecasts_df = pd.concat(forecast_dfs, axis=0)\n",
        "\n",
        "    # Reorder and rename columns as needed\n",
        "    columns = ['unique_id'] + [col for col in all_forecasts_df.columns if col != 'unique_id']\n",
        "    all_forecasts_df = all_forecasts_df[columns]\n",
        "\n",
        "    all_forecasts_df.columns.name = None\n",
        "\n",
        "    return all_forecasts_df\n",
        "\n",
        "def generate_time_series_dict(data, fct_periods, filter_data):\n",
        "    # Split train/test sets\n",
        "    test = data.groupby('unique_id').tail(fct_periods)\n",
        "    train = data.drop(test.index)\n",
        "\n",
        "    # Prepare time series dataframes\n",
        "    time_series_dfs = {uid: group for uid, group in train.groupby('unique_id')}\n",
        "    time_series_dict = {}\n",
        "\n",
        "    if filter_data:\n",
        "        # Filter out time series with insufficient non-zero data points\n",
        "        filtered_time_series_dfs = {}\n",
        "        for uid, group in time_series_dfs.items():\n",
        "            non_zero_index = group['y'].ne(0).idxmax()\n",
        "            start_index = max(0, non_zero_index - (13 - 1))\n",
        "            filtered_df = group.loc[non_zero_index:] if group.loc[non_zero_index:].shape[0] >= 13 else group.loc[start_index:]\n",
        "            if not filtered_df.empty:\n",
        "                filtered_time_series_dfs[uid] = filtered_df\n",
        "        # Convert each filtered DataFrame into a Darts TimeSeries object\n",
        "\n",
        "        time_series_dict = {uid: TimeSeries.from_dataframe(group, 'ds', 'y') for uid, group in filtered_time_series_dfs.items()}\n",
        "    else:\n",
        "        # Convert each original DataFrame into a Darts TimeSeries object without filtering\n",
        "        time_series_dict = {uid: TimeSeries.from_dataframe(group, 'ds', 'y') for uid, group in time_series_dfs.items()}\n",
        "\n",
        "    return time_series_dict\n",
        "\n",
        "def generate_forecast(data, fct_periods, model2use, filter_data=True):\n",
        "\n",
        "    # Use the nested function to generate the time series dictionary\n",
        "    time_series_dict = generate_time_series_dict(data, fct_periods, filter_data)\n",
        "\n",
        "    # Create and fit a model for each time series\n",
        "    models = {}\n",
        "    for uid, series in time_series_dict.items():\n",
        "        model = get_model(model2use)\n",
        "        model.fit(series)\n",
        "        models[uid] = model\n",
        "\n",
        "    # Forecasting\n",
        "    fct_dict = {uid: model.predict(fct_periods, num_samples=20) for uid, model in models.items()}\n",
        "    # Convert forecasts into a dataframe\n",
        "    fct_df = convert_fct2df(fct_dict)\n",
        "\n",
        "    return fct_dict, fct_df\n",
        "\n",
        "# Function to dynamically get the model instance\n",
        "def get_model(model_name):\n",
        "    if model_name == 'AutoETS':\n",
        "        return StatsForecastAutoETS()\n",
        "    elif model_name == 'ARIMA':\n",
        "        return StatsForecastAutoARIMA(season_length=12)\n",
        "    elif model_name == 'KF':\n",
        "        return KalmanForecaster(dim_x=12)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "# q = volume_act[volume_act['unique_id']=='Global/AMPHOTERCN - Amphotericin B/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/US10 - Astellas Pharma US, Inc.']\n",
        "\n",
        "ets_dict, ets_df = generate_forecast(volume_act[volume_act['unique_id'].isin(ets_ids)], fct_periods, model2use='AutoETS', filter_data=True)\n",
        "arima_dict, arima_df = generate_forecast(volume_act[volume_act['unique_id'].isin(arima_ids)], fct_periods, model2use='ARIMA', filter_data=True)\n",
        "# kf_fct = generate_forecast(volume_act, fct_periods, model2use='KF', filter_data=True)\n"
      ],
      "metadata": {
        "id": "hHFkuRx-53nl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "ded60b45-e47f-4f6e-aeac-49d328ae27d7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4745e763c46b>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# q = volume_act[volume_act['unique_id']=='Global/AMPHOTERCN - Amphotericin B/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/US10 - Astellas Pharma US, Inc.']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mets_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvolume_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mets_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfct_periods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AutoETS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0marima_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marima_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvolume_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marima_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfct_periods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2use\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ARIMA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# kf_fct = generate_forecast(volume_act, fct_periods, model2use='KF', filter_data=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-4745e763c46b>\u001b[0m in \u001b[0;36mgenerate_forecast\u001b[0;34m(data, fct_periods, model2use, filter_data)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mfct_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfct_periods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Convert forecasts into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mfct_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_fct2df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfct_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfct_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfct_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-4745e763c46b>\u001b[0m in \u001b[0;36mconvert_fct2df\u001b[0;34m(forecasts)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Concatenate all forecast DataFrames into a single DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mall_forecasts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Reorder and rename columns as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;36m1\u001b[0m   \u001b[0;36m3\u001b[0m   \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RUN QUARTERLY MODEL\n",
        "########################\n",
        "# Function to resample and sum data by quarter for each group\n",
        "def resample_group(group):\n",
        "    group = group.set_index('ds')  # Set 'ds' as the index\n",
        "    resampled_group = group.resample('Q').agg({'y': 'sum'})  # Aggregate data by quarter\n",
        "    return resampled_group\n",
        "\n",
        "quarterly_data = volume_act\n",
        "quarterly_data['ds'] = pd.to_datetime(quarterly_data['ds'])\n",
        "\n",
        "# Group by 'unique_id' and apply the resampling function\n",
        "quarterly_data = quarterly_data.groupby('unique_id').apply(resample_group).reset_index()\n",
        "\n"
      ],
      "metadata": {
        "id": "PYEIfAua_8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# XTREND - DECAY\n",
        "########################\n",
        "def apply_exponential_decay(df, start_date, end_date, end_value_percentage, target_unique_ids):\n",
        "    # Convert 'ds' to datetime if it's not already\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date = pd.to_datetime(end_date)\n",
        "\n",
        "    # Function to apply decay for each group\n",
        "    def decay_group(group):\n",
        "        # Only apply changes if unique_id is in target_unique_ids\n",
        "        if group['unique_id'].iloc[0] not in target_unique_ids:\n",
        "            return group\n",
        "\n",
        "        # Sort by date to ensure proper indexing\n",
        "        group = group.sort_values(by='ds')\n",
        "\n",
        "        # Columns to apply decay to\n",
        "        decay_columns = [col for col in group.columns if col not in ['unique_id', 'ds']]\n",
        "\n",
        "        # Initialize a dictionary to keep the end values for each decay column\n",
        "        end_values = {}\n",
        "\n",
        "        # Find start and end values and dates for each column\n",
        "        for col in decay_columns:\n",
        "            if start_date in group['ds'].values and end_date in group['ds'].values:\n",
        "                start_value = group.loc[group['ds'] == start_date, col].iloc[0]\n",
        "                end_value = start_value * end_value_percentage\n",
        "                end_values[col] = end_value  # Store the end value for this column\n",
        "\n",
        "                # Calculate the decay rate based on exponential decay formula\n",
        "                days = (end_date - start_date).days\n",
        "                decay_rate = np.log(end_value / start_value) / days\n",
        "\n",
        "                # Apply exponential decay for dates between start_date and end_date\n",
        "                for date in pd.date_range(start_date, end_date):\n",
        "                    if date in group['ds'].values:\n",
        "                        t = (date - start_date).days\n",
        "                        new_value = start_value * np.exp(decay_rate * t)\n",
        "                        group.loc[group['ds'] == date, col] = new_value\n",
        "\n",
        "        # Replace column values for dates after end_date with the respective end values\n",
        "        for col, end_value in end_values.items():\n",
        "            if end_value is not None:  # Ensure there was an end value calculated\n",
        "                group.loc[group['ds'] > end_date, col] = end_value\n",
        "\n",
        "        return group\n",
        "\n",
        "    # Apply the decay_group function to each group and return the modified dataframe\n",
        "    return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n",
        "\n",
        "# Apply exponential decay\n",
        "# lgbm_fct.rename(columns={'LGBM': 'y'}, inplace=True)\n",
        "ets_df.rename(columns={'ETS': 'y'}, inplace=True)\n",
        "arima_df.rename(columns={'ARIMA': 'y'}, inplace=True)\n",
        "\n",
        "# Micafungin\n",
        "arima_df = apply_exponential_decay(arima_df, '2023-07-01', '2023-08-01', 0, divested_ids)\n",
        "ets_df = apply_exponential_decay(ets_df, '2023-07-01', '2023-08-01', 0, divested_ids)\n",
        "\n",
        "# Lexiscan\n",
        "arima_df = apply_exponential_decay(arima_df, '2023-04-01', '2023-12-01', .1, loe_ids)\n",
        "ets_df = apply_exponential_decay(ets_df, '2023-04-01', '2023-12-01', .1, loe_ids)\n",
        "\n",
        "# # Tamsulosin\n",
        "# tamsulosin_ids = volume_act[volume_act['ProductLv'].isin(['TAMSULOSIN - Tamsulosin HCl', 'TAMSUL_TAB - Tamsulosin tab'])]['unique_id'].unique()\n",
        "# arima_df = apply_exponential_decay(arima_df, '2023-04-01', '2023-12-01', .9, tamsulosin_ids)\n",
        "# ets_df = apply_exponential_decay(ets_df, '2023-04-01', '2023-12-01', .9, tamsulosin_ids)\n",
        "\n",
        "# Solifinacin Tamsulosin\n",
        "solif_tams_ids = volume_act[(volume_act['ProductLv'].isin(['SOLIF_TAMS - Solifenacin _ Tamsulosin'])) & (volume_act['Lv5'].isin(['D_E_PORTUGAL - Portugal', 'D_E_SPAIN - Spain', 'D_E_GB - Great Britain', 'D_E_BG - Bulgaria']))]['unique_id'].unique()\n",
        "arima_df = apply_exponential_decay(arima_df, '2023-06-01', '2023-12-01', .7, solif_tams_ids)\n",
        "ets_df = apply_exponential_decay(ets_df, '2023-06-01', '2023-12-01', .7, solif_tams_ids)"
      ],
      "metadata": {
        "id": "y-UfqmzMxgIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# XTREND - GROWTH\n",
        "########################\n",
        "# volume_act = volume_act[~volume_act['unique_id'].isin(new_ids)]\n"
      ],
      "metadata": {
        "id": "N3pMlJne3M5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAbo6hYuCf3O"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "# METRICS\n",
        "########################\n",
        "# Subset\n",
        "volume_act_xsm = volume_act[['unique_id', 'ds', 'y']]\n",
        "budget2 = budget[['unique_id', 'ds', 'y']]\n",
        "ets_df2 = ets_df[['unique_id', 'ds', 'y']]\n",
        "arima_df2 = arima_df[['unique_id', 'ds', 'y']]\n",
        "\n",
        "# Assign names\n",
        "volume_act_xsm.rename(columns={'y': 'Actuals'}, inplace=True)\n",
        "budget2.rename(columns={'y': 'Budget'}, inplace=True)\n",
        "ets_df2.rename(columns={'y': 'ETS'}, inplace=True)\n",
        "arima_df2.rename(columns={'y': 'ARIMA'}, inplace=True)\n",
        "\n",
        "# Merge actuals, budget and forecast\n",
        "rev_at = volume_act_xsm.merge(ets_df2, on=['unique_id', 'ds'], how='left')\n",
        "rev_at = rev_at.merge(budget2, on=['unique_id', 'ds'], how='left')\n",
        "rev_at = rev_at.merge(arima_df2, on=['unique_id', 'ds'], how='left')\n",
        "\n",
        "# Conditions for selection\n",
        "# conditions = [rev_at['unique_id'].isin(spend_ids),rev_at['unique_id'].isin(arima_ids),rev_at['unique_id'].isin(ets_ids)]\n",
        "# choices = [rev_at['ARIMA'], rev_at['ARIMA'],rev_at['ETS']]\n",
        "\n",
        "conditions = [rev_at['unique_id'].isin(arima_ids),rev_at['unique_id'].isin(ets_ids)]\n",
        "choices = [rev_at['ARIMA'],rev_at['ETS']]\n",
        "\n",
        "# Creating the new column 'SelectedFCT' based on the conditions\n",
        "rev_at['SelectedFCT'] = np.select(conditions, choices, default=np.nan)\n",
        "\n",
        "# Only keep tested ts\n",
        "rev_at = rev_at[rev_at['unique_id'].isin(tested_ts)]\n",
        "\n",
        "# Filter for dates\n",
        "data4metrics = rev_at[(rev_at['ds']<=fct_end_date) & (rev_at['ds']>=fct_st_date)]\n",
        "\n",
        "# Sum up the values for each unique_id\n",
        "numeric_cols = data4metrics.columns.drop(['unique_id', 'ds'])\n",
        "summed_df = data4metrics.groupby('unique_id')[numeric_cols].sum()\n",
        "\n",
        "# Calculate difference and percentage differences from 'Actuals'\n",
        "absolute_diff = summed_df.subtract(summed_df['Actuals'], axis=0).abs()\n",
        "percentage_diff = summed_df.subtract(summed_df['Actuals'], axis=0).div(summed_df['Actuals'], axis=0).abs()\n",
        "\n",
        "# Drop the 'Actuals' column as we don't need to compare it with itself\n",
        "absolute_diff.drop(columns=['Actuals', 'ARIMA', 'ETS'], inplace=True)\n",
        "\n",
        "# Find the column with the lowest difference for each unique_id and add to metrics table\n",
        "min_diff_col = absolute_diff.idxmin(axis=1)\n",
        "data4metrics['lowest_diff_col'] = data4metrics['unique_id'].map(min_diff_col)\n",
        "\n",
        "# Find winner\n",
        "winner = data4metrics.groupby('lowest_diff_col')\n",
        "\n",
        "# Get Budget winners\n",
        "bud_winners = winner.get_group('Budget')['unique_id'].unique()\n",
        "\n",
        "winner['unique_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUFeFJhSRob_"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "# CREATE PLOT DATA\n",
        "########################\n",
        "fct_st_date = pd.to_datetime(fct_st_date)\n",
        "\n",
        "# Add revenue actuals\n",
        "data2plot = rev_at.copy()\n",
        "data2plot['ds'] = pd.to_datetime(data2plot['ds'])\n",
        "\n",
        "# Update Actuals columns\n",
        "data2plot['Actuals (Train)'] = data2plot['Actuals'].copy()\n",
        "data2plot['Actuals'] = data2plot.apply(lambda row: row['Actuals'] if row['ds'] >= fct_st_date else None, axis=1)\n",
        "data2plot['Actuals (Train)'] = data2plot.apply(lambda row: row['Actuals (Train)'] if row['ds'] < fct_st_date else None, axis=1)\n",
        "\n",
        "# Filter to end date\n",
        "data2plot = data2plot[data2plot['ds']<=fct_end_date]\n",
        "\n",
        "# Find TS to fix\n",
        "ts2fix = data2plot[data2plot['unique_id'].isin(bud_winners)]\n",
        "tsnonspend = data2plot[data2plot['unique_id'].isin(non_spend_ids)]\n",
        "\n",
        "data2plot.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# CREATE HIERARCHICAL DATAFRAMES\n",
        "########################\n",
        "def split_unique_id_into_columns(df, column_name):\n",
        "\n",
        "    # Split 'unique_id' into 4 new columns\n",
        "    split_columns = df[column_name].str.split('/', expand=True)\n",
        "\n",
        "    # Rename the columns\n",
        "    split_columns.columns = hierarchy_levels[-1]\n",
        "\n",
        "    # Join back to original dataframe\n",
        "    result_df = pd.concat([df, split_columns], axis=1)\n",
        "\n",
        "    # Check for columns that are not in split_columns, 'unique_id' or 'ds'\n",
        "    for col in result_df.columns:\n",
        "        if col not in hierarchy_levels[-1] + ['unique_id', 'ds']:\n",
        "            # Rename the column to 'y'\n",
        "            result_df = result_df.rename(columns={col: 'y'})\n",
        "            break  # Assuming only one column needs to be renamed\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def create_hier(data, hierarchy_levels, tested_ts, fct_periods):\n",
        "\n",
        "    # Filter data based on tested_ts\n",
        "    data_filtered = data[data['unique_id'].isin(tested_ts)]\n",
        "\n",
        "    # Identify the columns to aggregate\n",
        "    columns_to_aggregate = [col for col in data_filtered.columns if col not in (hierarchy_levels[-1] + ['unique_id', 'ds'])]\n",
        "\n",
        "    hier_final = data_filtered[['unique_id', 'ds']]\n",
        "    # Fill NA values for these columns\n",
        "    for col in columns_to_aggregate:\n",
        "        data_quantile = data_filtered[(hierarchy_levels[-1] + ['unique_id', 'ds']+[col])]\n",
        "        data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
        "        data_quantile['y'] = data_quantile['y'].fillna(0)\n",
        "\n",
        "        hier, S_df, tags = aggregate(df=data_quantile, spec=hierarchy_levels)\n",
        "\n",
        "        hier = hier.reset_index()\n",
        "        hier.rename(columns={'y': col}, inplace=True)\n",
        "        hier_final = hier_final.merge(hier, on = ['unique_id', 'ds'], how = 'right')\n",
        "    # Split the data into train and test sets\n",
        "    test = hier_final.groupby('unique_id').tail(fct_periods)\n",
        "    train = hier_final.drop(test.index)\n",
        "\n",
        "    return train, test, S_df, tags\n",
        "\n",
        "# Create hierarchies for forecast, actuals and budget\n",
        "rev_fct = split_unique_id_into_columns(rev_at[rev_at['ds']>=fct_st_date][['unique_id', 'ds', 'SelectedFCT']], 'unique_id')\n",
        "rev_act = split_unique_id_into_columns(rev_at[['unique_id', 'ds', 'Actuals']], 'unique_id')\n",
        "rev_bud = split_unique_id_into_columns(rev_at[rev_at['ds']>=fct_st_date][['unique_id', 'ds', 'Budget']], 'unique_id')\n",
        "\n",
        "revf_train, revf_test, S_df, tags = create_hier(rev_fct, hierarchy_levels, tested_ts, fct_periods)\n",
        "reva_train, reva_test, S_df, tags = create_hier(rev_act, hierarchy_levels, tested_ts, fct_periods)\n",
        "bud_train, bud_test, S_df, tags = create_hier(rev_bud, hierarchy_levels, tested_ts, fct_periods)"
      ],
      "metadata": {
        "id": "Wcre0Pj3HjM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# HIERARCHICAL RECONCILIATION FOR POINT FORECAST\n",
        "########################\n",
        "# # Select reconcilers\n",
        "# reconcilers = [\n",
        "#     TopDown(method='forecast_proportions')\n",
        "#     # OptimalCombination(method = 'ols', nonnegative=True)\n",
        "#     # BottomUp()\n",
        "#     # ERM(method='closed')\n",
        "# ]\n",
        "\n",
        "# # Rename y to model name\n",
        "# revf_test.rename(columns={'y': 'model'}, inplace=True)\n",
        "\n",
        "# # Reconcile the base predictions\n",
        "# hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
        "# revf_rec = hrec.reconcile(Y_hat_df=revf_test.set_index('unique_id'), Y_df=reva_train.set_index('unique_id'),\n",
        "#                           S=S_df, tags=tags)\n",
        "\n",
        "# # Reset Index and columns\n",
        "# revf_rec = revf_rec[['ds', revf_rec.columns[2]]]\n",
        "# revf_rec = revf_rec.reset_index()\n",
        "# revf_rec.columns = ['unique_id', 'ds', 'Forecast_H']"
      ],
      "metadata": {
        "id": "N4iMkTjqI315"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volume_act[volume_act['ProductLv'].isin(['ENZA - Enzalutamide','MIRABEGRON - Mirabegron'])]['unique_id'].unique()"
      ],
      "metadata": {
        "id": "-qLzMovk7yEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ids= ['Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_HUBGROGR - HBRG/D_E_HUBGRO - Hungary  Bulgaria & Romania/D_E_BG - Bulgaria',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ADRCS_PT - Adriatics & Portugal/D_E_ADRCS - Adriatic Adriatics/D_E_CROATIA - Croatia',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BBMCI - BBMCI group/D_E_MTCYIS - Malta  Cyprus & Iceland/D_E_CYPRUS - Cyprus',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BBMCI - BBMCI group/D_E_MTCYIS - Malta  Cyprus & Iceland/D_E_CYPRUS - Cyprus',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_CZECH - Czech',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_CZECH - Czech',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_DE - Germany/D_E_DE - Germany/D_E_DE - Germany/D_E_DE - Germany',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_DE - Germany/D_E_DE - Germany/D_E_DE - Germany/D_E_DE - Germany',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_NORDIC - Nordic/D_E_DENMARK - Denmark/D_E_DENMARK - Denmark',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_NORDIC - Nordic/D_E_FINLAND - Finland/D_E_FINLAND - Finland',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France/D_E_FRANCE - France',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain/D_E_GB - Great Britain',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_HUBGROGR - HBRG/D_E_GREECE - Greece/D_E_GREECE - Greece',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_IE - Ireland/D_E_IE - Ireland/D_E_IE - Ireland',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_IL - Israel/D_E_IL - Israel/D_E_IL - Israel',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy/D_E_IT - Italy',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_BENELUX - Benelux/D_E_NETHLND - Netherlands/D_E_NETHLND - Netherlands',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_NORDIC - Nordic/D_E_NORWAY - Norway/D_E_NORWAY - Norway',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ADRCS_PT - Adriatics & Portugal/D_E_PORTUGAL - Portugal/D_E_PORTUGAL - Portugal',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_CZSK - Czech + Slovakia/D_E_SLOVAKIA - Slovakia',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ADRCS_PT - Adriatics & Portugal/D_E_ADRCS - Adriatic Adriatics/D_E_SLVNA - Slovenia',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_SPAIN - Spain/D_E_SPAIN - Spain/D_E_SPAIN - Spain/D_E_SPAIN - Spain',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_NORDIC - Nordic/D_E_SWEDEN - Sweden/D_E_SWEDEN - Sweden',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_ALPINE - Alpine/D_E_SWITZ - Switzerland/D_E_SWITZ - Switzerland',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_E_ESTMKT - Established Markets/D_E_MSM - Mid Size Markets/D_E_PCSU - PCSU/D_E_UA - Ukraine/D_E_UA - Ukraine',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_KSAUAE - Saudi Arabia and UAE/D_I_AE - UAE/D_I_AE - UAE',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_RBK_CORE - RBK Core/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia/D_I_CIS_RUS - Russia',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_EGYPT - Egypt/D_I_EGYPT - Egypt/D_I_EGYPT - Egypt',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_DB - Distributor Business/D_I_KUWAIT - Kuwait/D_I_KUWAIT - Kuwait/D_I_KUWAIT - Kuwait',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_CORE - MEA_CORE/D_I_MEA_CORE - MEA_CORE/D_I_MEA_CORE - MEA_CORE/D_I_MEA_CORE - MEA_CORE',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_KSAUAE - Saudi Arabia and UAE/D_I_SA - Saudi Arabia/D_I_SA - Saudi Arabia',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_MEA_OB - MEA Own Business/D_I_SAFRICA - South Africa/D_I_SAFRICA - South Africa/D_I_SAFRICA - South Africa',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_I_INTL - International Markets/D_I_APAC_CORE - APAC CORE/D_I_SINMAL - SINMAL/D_I_SINGAPORE - SINGAPORE/D_I_SINGAPORE - SINGAPORE',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_I_INTL - International Markets/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey/D_I_TURKEY - Turkey',\n",
        " 'Global/ROXADUSTNT - Roxadustant/D_JPCOM - Japan Commercial/D_JPCOM - Japan Commercial/D_JPCOM - Japan Commercial/D_JPCOM - Japan Commercial/JP10 - Astellas Pharma Inc',\n",
        " 'Global/ENFORTUMAB - Enforumab Vedotin/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/D_USCOM - US Commercial/US21 - Agensys, Inc.']\n",
        "\n"
      ],
      "metadata": {
        "id": "_oJ6qnE-MjlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# HIERARCHICAL RECONCILIATION FOR PROBABILISTIC FORECAST\n",
        "########################\n",
        "quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995] # Define your quantiles\n",
        "weights = {0.005: 1, 0.025: 1, 0.165: 1, 0.250: 1, 0.500: 1, 0.750: 1, 0.835: 1, 0.975: 1, 0.995: 1}\n",
        "\n",
        "# Get ranges\n",
        "Y_hier_df, S_df, tags = aggregate(df=volume_act, spec=hierarchy_levels)\n",
        "Y_hier_df = Y_hier_df.reset_index()\n",
        "\n",
        "#split train/test sets\n",
        "Y_test_df  = Y_hier_df.groupby('unique_id').tail(fct_periods)\n",
        "Y_train_df = Y_hier_df.drop(Y_test_df.index)\n",
        "\n",
        "# Compute base predictions\n",
        "fcst = StatsForecast(df=Y_train_df,\n",
        "                     models=[AutoETS(season_length=12)],\n",
        "                     freq='MS', n_jobs=-1)\n",
        "\n",
        "Y_hat_df = fcst.forecast(h=fct_periods, fitted=True, level=quantiles)\n",
        "\n",
        "Y_fitted_df = fcst.forecast_fitted_values()\n",
        "Y_fitted_df = Y_fitted_df[['unique_id', 'ds', 'y', 'AutoETS', 'AutoETS-lo-0.995','AutoETS-lo-0.975', 'AutoETS-lo-0.835', 'AutoETS-lo-0.75', 'AutoETS-hi-0.75', 'AutoETS-hi-0.835','AutoETS-hi-0.975', 'AutoETS-hi-0.995']]\n",
        "Y_fitted_df.columns = ['unique_id', 'ds', 'y', 'model', 'model-lo-99.5','model-lo-97.5', 'model-lo-83.5', 'model-lo-75', 'model-hi-75', 'model-hi-83.5','model-hi-97.5', 'model-hi-99.5']\n",
        "\n",
        "# Create probabilistic dataframe\n",
        "arima_df.rename(columns={'ARIMA': 'y'}, inplace=True)\n",
        "ets_df.rename(columns={'ETS': 'y'}, inplace=True)\n",
        "\n",
        "arima_dfp = arima_df[arima_df['unique_id'].isin(arima_ids)]\n",
        "ets_dfp = ets_df[ets_df['unique_id'].isin(ets_ids)]\n",
        "\n",
        "rev_prb = pd.concat([arima_dfp, ets_dfp])\n",
        "\n",
        "# NEW PRODUCTS: EV AND ROXA\n",
        "rev_prb1=rev_prb[rev_prb['unique_id'].isin(new_ids)]\n",
        "rev_prb1['y_0.005'] = rev_prb1['y_0.005']*1.4\n",
        "rev_prb1['y_0.025'] = rev_prb1['y_0.025']*1.4\n",
        "rev_prb1['y_0.165'] = rev_prb1['y_0.165']*1.4\n",
        "rev_prb1['y_0.25'] = rev_prb1['y_0.25']*1.4\n",
        "rev_prb1['y'] = rev_prb1['y']*1.4\n",
        "rev_prb1['y_0.75'] = rev_prb1['y_0.75']*1.4\n",
        "rev_prb1['y_0.835'] = rev_prb1['y_0.835']*1.4\n",
        "rev_prb1['y_0.975'] = rev_prb1['y_0.975']*1.4\n",
        "rev_prb1['y_0.995'] = rev_prb1['y_0.995']*1.4\n",
        "\n",
        "rev_prb2=rev_prb[~rev_prb['unique_id'].isin(new_ids)]\n",
        "\n",
        "rev_prb = pd.concat([rev_prb1, rev_prb2]).reset_index(drop=True)\n",
        "\n",
        "# NEW INDICATION: ENZA AND MIRA\n",
        "ni_enza_roxa_list = volume_act[volume_act['ProductLv'].isin(['ENZA - Enzalutamide','MIRABEGRON - Mirabegron'])]['unique_id'].unique()\n",
        "rev_prb1=rev_prb[rev_prb['unique_id'].isin(ni_enza_roxa_list)]\n",
        "rev_prb1['y_0.005'] = rev_prb1['y_0.005']*1.1\n",
        "rev_prb1['y_0.025'] = rev_prb1['y_0.025']*1.1\n",
        "rev_prb1['y_0.165'] = rev_prb1['y_0.165']*1.1\n",
        "rev_prb1['y_0.25'] = rev_prb1['y_0.25']*1.1\n",
        "rev_prb1['y'] = rev_prb1['y']*1.1\n",
        "rev_prb1['y_0.75'] = rev_prb1['y_0.75']*1.1\n",
        "rev_prb1['y_0.835'] = rev_prb1['y_0.835']*1.1\n",
        "rev_prb1['y_0.975'] = rev_prb1['y_0.975']*1.1\n",
        "rev_prb1['y_0.995'] = rev_prb1['y_0.995']*1.1\n",
        "\n",
        "rev_prb2=rev_prb[~rev_prb['unique_id'].isin(ni_enza_roxa_list)]\n",
        "\n",
        "rev_prb = pd.concat([rev_prb1, rev_prb2]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Split 'unique_id' into 4 new columns\n",
        "split_columns = rev_prb['unique_id'].str.split('/', expand=True)\n",
        "\n",
        "# Rename the columns\n",
        "split_columns.columns = hierarchy_levels[-1]\n",
        "rev_prb = pd.concat([rev_prb, split_columns], axis=1)\n",
        "\n",
        "# Filter\n",
        "rev_prb = rev_prb[rev_prb['unique_id'].isin(tested_ts)]\n",
        "\n",
        "rev_prb_train, rev_prb_test, S_df, tags = create_hier(rev_prb, hierarchy_levels, tested_ts, fct_periods)\n",
        "\n",
        "\n",
        "reconcilers = [\n",
        "    BottomUp(),\n",
        "# TopDown(method='forecast_proportions')\n",
        "]\n",
        "\n",
        "# Rename y to model name\n",
        "rev_prb_test.columns = ['unique_id', 'ds', 'model-lo-99.5','model-lo-97.5', 'model-lo-83.5', 'model-lo-75', 'model', 'model-hi-75', 'model-hi-83.5','model-hi-97.5', 'model-hi-99.5']\n",
        "\n",
        "Y_fitted_df = Y_fitted_df[Y_fitted_df['unique_id'].isin(rev_prb_test['unique_id'])]\n",
        "\n",
        "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
        "rev_prb_rec = hrec.reconcile(Y_hat_df=rev_prb_test.set_index('unique_id'), Y_df=Y_fitted_df.set_index('unique_id'),\n",
        "                          S=S_df, tags=tags,level=[75, 83.5, 97.5, 99.5], intervals_method='normality')\n",
        "\n",
        "rev_prb_rec.rename(columns={'model': 'Forecast'}, inplace=True)\n",
        "\n",
        "rev_prb_rec = rev_prb_rec.reset_index()"
      ],
      "metadata": {
        "id": "1P6M_3GqR7tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# CREATE DATAFRAME TO PLOT\n",
        "########################\n",
        "bud_test.rename(columns={'y': 'Budget'}, inplace=True)\n",
        "\n",
        "# Update Actuals columns\n",
        "reva = pd.concat([reva_train, reva_test])\n",
        "reva['Actuals'] = reva.apply(lambda row: row['y'] if row['ds'] >= fct_st_date else None, axis=1)\n",
        "reva['Actuals (Train)'] = reva.apply(lambda row: row['y'] if row['ds'] < fct_st_date else None, axis=1)\n",
        "\n",
        "# Update forecast\n",
        "rev_prb_rec = rev_prb_rec[rev_prb_rec['ds']>=fct_st_date]\n",
        "\n",
        "# Merge\n",
        "rev_at_hier = reva.merge(rev_prb_rec[['unique_id', 'ds', 'Forecast']], on=['unique_id', 'ds'], how='left')\n",
        "rev_at_hier = rev_at_hier.merge(bud_test, on=['unique_id', 'ds'], how='left')\n",
        "rev_at_hier2plot = rev_at_hier.drop(columns=['y'])\n",
        "\n",
        "rev_prb_rec = rev_prb_rec[['unique_id', 'ds', 'model-lo-99.5', 'model-lo-97.5', 'model-lo-83.5',\n",
        "       'model-lo-75', 'Forecast', 'model-hi-75', 'model-hi-83.5',\n",
        "       'model-hi-97.5', 'model-hi-99.5']]"
      ],
      "metadata": {
        "id": "p38A2zrxJK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# METRICS\n",
        "########################\n",
        "\n",
        "def sum_of_differences(time_series):\n",
        "    # Find the index of the first non-zero value\n",
        "    first_non_zero_index = next((index for index, value in enumerate(time_series) if value != 0), None)\n",
        "\n",
        "    # Check if there is a non-zero value in the series\n",
        "    if first_non_zero_index is None:\n",
        "        return 0  # Return 0 if there are no non-zero values\n",
        "\n",
        "    # Calculate the sum of the differences after the first non-zero value\n",
        "    sum_diff = sum(abs(time_series[i] - time_series[i - 1]) for i in range(first_non_zero_index + 1, len(time_series)))\n",
        "\n",
        "    # Calculate the number of time points after the first non-zero value minus one\n",
        "    num_points = len(time_series) - first_non_zero_index - 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if num_points <= 0:\n",
        "        return 0\n",
        "\n",
        "    # Return the result\n",
        "    return np.array((sum_diff / num_points).values())[0][0]\n",
        "\n",
        "\n",
        "def metrics_calculation(actual_data, forecasted_data, quantiles, weights, sample_columns, stochastic=False):\n",
        "    # Prepare a list to store WSPL results for each unique_id\n",
        "    results = []\n",
        "\n",
        "    # Ensure 'ds' is in datetime format\n",
        "    actual_data['ds'] = pd.to_datetime(actual_data['ds'])\n",
        "\n",
        "    for unique_id in actual_data['unique_id'].unique():\n",
        "        try:\n",
        "            wspl, rmse_metric, rmsse_metric, ope_metric = np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "            # Filter the actual data\n",
        "            actual_values = actual_data[(actual_data['unique_id'] == unique_id) & (actual_data['ds'] >= fct_st_date)][['ds', 'y']].tail(fct_periods)\n",
        "\n",
        "            actual_ts = TimeSeries.from_dataframe(actual_values.set_index('ds'))\n",
        "\n",
        "            historical_actuals = actual_data[actual_data['unique_id'] == unique_id][['ds', 'y']].drop(actual_values.index)\n",
        "\n",
        "            historical_ts = TimeSeries.from_dataframe(historical_actuals.set_index('ds'))\n",
        "\n",
        "            # Filter the forecasted data\n",
        "            forecasted_values = forecasted_data[forecasted_data['unique_id']==unique_id]\n",
        "\n",
        "            forecasted_values = forecasted_values.sort_values('ds')\n",
        "\n",
        "            # Find the unique time points\n",
        "            unique_times = forecasted_values['ds'].unique()\n",
        "            num_times = len(unique_times)\n",
        "\n",
        "            # Define the number of components and samples\n",
        "            num_components = 1  # 'y'\n",
        "            num_samples = len(sample_columns)   # Number of forecast columns\n",
        "\n",
        "            # Initialize the 3D array\n",
        "            array_3d = np.zeros((num_times, num_components, num_samples))\n",
        "\n",
        "            # Fill in the array\n",
        "            for i, time in enumerate(unique_times):\n",
        "                # Select the corresponding rows from the DataFrame\n",
        "                row = forecasted_values[forecasted_values['ds'] == time]\n",
        "                # Extract the sample values and assign them to the array\n",
        "                samples = row[sample_columns].to_numpy().reshape(num_samples)\n",
        "                array_3d[i, 0, :] = samples\n",
        "\n",
        "            # Convert the 'ds' column to datetime\n",
        "            forecasted_values['ds'] = pd.to_datetime(forecasted_values['ds'])\n",
        "\n",
        "            # Create a DatetimeIndex from the 'ds' column\n",
        "            datetime_index = pd.DatetimeIndex(forecasted_values['ds'])\n",
        "            forecasted_ts = TimeSeries.from_times_and_values(datetime_index, array_3d)\n",
        "\n",
        "            # Get Scale\n",
        "            scaled = sum_of_differences(historical_ts)\n",
        "\n",
        "            # Initialize losses dictionary\n",
        "            losses = {}\n",
        "\n",
        "            if stochastic:\n",
        "                # Calculate quantile loss for each quantile if stochastic is True\n",
        "                for q in quantiles:\n",
        "                    try:\n",
        "                        losses[q] = quantile_loss(actual_ts, forecasted_ts, q)\n",
        "                    except Exception as e:  # Use appropriate exception handling based on your quantile_loss function\n",
        "                        print(f\"Error calculating quantile loss for {q}: {e}\")\n",
        "                        losses[q] = np.nan\n",
        "            else:\n",
        "                # Set all losses to NaN if stochastic is False\n",
        "                for q in quantiles:\n",
        "                    losses[q] = np.nan\n",
        "\n",
        "            # Pinball Loss\n",
        "            pl = sum(weights[q] * losses.get(q, np.nan) for q in quantiles) / sum(weights.values()) if stochastic else np.nan\n",
        "\n",
        "            # Scaled Pinball Loss\n",
        "            spl = pl/scaled if scaled != 0 else np.nan\n",
        "\n",
        "            # RMSE\n",
        "            mse_metric = mse(actual_ts, forecasted_ts)\n",
        "            rmse_metric = np.sqrt(mse_metric)\n",
        "\n",
        "            # RMSSE\n",
        "            rmsse_metric = np.sqrt((mse_metric / (scaled**2))) if scaled != 0 else np.nan\n",
        "\n",
        "            # Overall Percentage Error\n",
        "            try:\n",
        "                ope_metric = ope(actual_ts, forecasted_ts)\n",
        "            except Exception as e:\n",
        "                ope_metric = np.nan\n",
        "\n",
        "            # Append the result to the list\n",
        "            results.append({'unique_id': unique_id, 'PL': pl, 'SPL': spl, 'RMSE': rmse_metric, 'RMSSE': rmsse_metric, 'OPE': ope_metric})\n",
        "\n",
        "        except Exception as e:\n",
        "            # If an error occurs, log it, and append NaN metrics for this unique_id\n",
        "            print(f\"An error occurred for {unique_id}: {e}\")\n",
        "            results.append({'unique_id': unique_id, 'PL': np.nan, 'SPL': np.nan, 'RMSE': np.nan, 'RMSSE': np.nan, 'OPE': np.nan})\n",
        "\n",
        "    # Convert the list of results into a DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Usage:\n",
        "quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995] # Define your quantiles\n",
        "weights = {0.005: 1, 0.025: 1, 0.165: 1, 0.250: 1, 0.500: 1, 0.750: 1, 0.835: 1, 0.975: 1, 0.995: 1}\n",
        "sample_columns_fct = ['model-lo-99.5', 'model-lo-97.5', 'model-lo-83.5', 'model-lo-75','Forecast', 'model-hi-75', 'model-hi-83.5', 'model-hi-97.5','model-hi-99.5']\n",
        "\n",
        "\n",
        "rev_at_hier = rev_at_hier[rev_at_hier['unique_id'].isin(rev_prb_rec['unique_id'])]\n",
        "\n",
        "fct_results = metrics_calculation(rev_at_hier[['unique_id', 'ds', 'y']], rev_prb_rec, quantiles, weights, sample_columns_fct, stochastic=True)\n",
        "bud_results = metrics_calculation(rev_at_hier[['unique_id', 'ds', 'y']], bud_test, quantiles, weights, ['Budget'], stochastic=False)"
      ],
      "metadata": {
        "id": "aHRgSwkvceoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RESULTS DATAFRAME\n",
        "########################\n",
        "# Ensure 'ds' is in datetime format\n",
        "rev_at_hier['ds'] = pd.to_datetime(rev_at_hier['ds'])\n",
        "\n",
        "# Filter the DataFrame for dates within the specified range\n",
        "results = rev_at_hier[(rev_at_hier['ds'] >= fct_st_date) & (rev_at_hier['ds'] <= fct_end_date)]\n",
        "results = results.groupby('unique_id')[['Actuals', 'Budget', 'Forecast']].sum().reset_index()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def create_output(results, fct_results, metrics, col_nme, sbm_products):\n",
        "    # Merge results with forecast results\n",
        "    results_fct = results[['unique_id', 'Actuals', col_nme]].merge(fct_results, how='left', on='unique_id')\n",
        "\n",
        "    # Split 'unique_id' into separate levels\n",
        "    split_columns = results_fct['unique_id'].str.split('/', expand=True)\n",
        "    split_columns.columns = hierarchy_levels[-1]\n",
        "\n",
        "    # Concatenate split columns back to the original dataframe\n",
        "    results_fct = pd.concat([results_fct, split_columns], axis=1)\n",
        "\n",
        "    # Rearrange and rename columns\n",
        "    results_fct = results_fct[['unique_id'] + hierarchy_levels[-1] + ['Actuals', col_nme, 'PL', 'SPL', 'RMSE', 'RMSSE', 'OPE']]\n",
        "    results_fct.columns = ['unique_id'] + hierarchy_levels[-1] + ['Actuals', col_nme, 'PL', 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "\n",
        "    for column in hierarchy_levels[-1]:\n",
        "        results_fct[column] = results_fct[column].fillna('')\n",
        "\n",
        "    # Filter for Product Level where 'Region' is empty\n",
        "    results_fct_prod = results_fct[results_fct['Lv1'] == ''][hierarchy_levels[1] + ['Actuals', col_nme, 'RMSE', 'RMSSE', 'Error %', 'PL', 'SPL']]\n",
        "    results_fct_prod['Product Category'] = results_fct['ProductLv'].apply(lambda x: 'SBM' if x in sbm_products else 'Top Product & Others')\n",
        "    results_fct_prod = results_fct_prod[['Product Category', 'TopLv', 'ProductLv', 'Actuals', col_nme, 'RMSE', 'RMSSE', 'Error %', 'PL', 'SPL']]\n",
        "\n",
        "    # Add new 'Product Category' column based on 'ProductLv' and sbm_products list\n",
        "    results_fct['Product Category'] = results_fct['ProductLv'].apply(lambda x: 'SBM' if x in sbm_products else 'Top Product & Others')\n",
        "\n",
        "\n",
        "    # Initialize a dictionary to hold the pivoted DataFrames for each metric\n",
        "    pivot_dfs = {}\n",
        "\n",
        "    for metric in metrics:\n",
        "        pivot_df = results_fct.pivot_table(\n",
        "            index=['Product Category'] + hierarchy_levels[1],\n",
        "            columns=hierarchy_levels[-1][-5:],\n",
        "            values=metric,\n",
        "            aggfunc='sum'  # Change as needed\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add the pivoted DataFrame to the dictionary\n",
        "        pivot_dfs[metric] = pivot_df\n",
        "\n",
        "    return pivot_dfs, results_fct_prod\n",
        "\n",
        "\n",
        "sbm_products = ['ENZA - Enzalutamide', 'GILTERITNB - Gilteritinib', 'ENFORTUMAB - Enforumab Vedotin', 'ROXADUSTNT - Roxadustant', 'FEZO - Fezolinetant']\n",
        "\n",
        "# Forecast\n",
        "metrics = ['Actuals', 'Forecast', 'PL', 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "pivot_dfs, results_fct_prod = create_output(results, fct_results, metrics, 'Forecast', sbm_products)\n",
        "actuals_df = pivot_dfs['Actuals']\n",
        "output_df = pivot_dfs['Forecast']\n",
        "rmse_df = pivot_dfs['RMSE']\n",
        "rmsse_df = pivot_dfs['RMSSE']\n",
        "ope_df = pivot_dfs['Error %']\n",
        "pl_df = pivot_dfs['PL']\n",
        "spl_df = pivot_dfs['SPL']\n",
        "\n",
        "# Budget\n",
        "metrics = ['Actuals', 'Budget', 'PL', 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "pivot_dfs2, results_bud_prod = create_output(results, bud_results, metrics, 'Budget', sbm_products)\n",
        "actuals_df2 = pivot_dfs2['Actuals']\n",
        "output_df2 = pivot_dfs2['Budget']\n",
        "rmse_df2 = pivot_dfs2['RMSE']\n",
        "rmsse_df2 = pivot_dfs2['RMSSE']\n",
        "ope_df2 = pivot_dfs2['Error %']\n",
        "pl_df2 = pivot_dfs['PL']\n",
        "spl_df2 = pivot_dfs2['SPL']\n",
        "\n",
        "# Get column starts\n",
        "col_starts = [1, results_fct_prod.shape[1]+1, output_df.shape[1]+2, rmse_df.shape[1]+2, rmsse_df.shape[1]+2, ope_df.shape[1]+2, pl_df.shape[1]+2]\n",
        "col_starts_sum = []\n",
        "running_total = 0\n",
        "\n",
        "for value in col_starts:\n",
        "    running_total += value\n",
        "    col_starts_sum.append(running_total)\n",
        "\n",
        "\n",
        "# # Create a Pandas Excel writer using openpyxl as the engine\n",
        "# filename='/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/consolidated_results.xlsx'\n",
        "# with pd.ExcelWriter(filename) as writer:\n",
        "#     results_fct_prod.to_excel(writer, sheet_name='Sheet1', startrow=3, startcol=col_starts_sum[0], header=True, index=False)\n",
        "#     output_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[1], header=True)\n",
        "#     rmsse_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[2], header=True)\n",
        "#     ope_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[3], header=True)\n",
        "#     spl_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[4], header=True)\n",
        "\n",
        "#     results_bud_prod.to_excel(writer, sheet_name='Sheet2', startrow=3, startcol=col_starts_sum[0], header=True, index=False)\n",
        "#     output_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[1], header=True)\n",
        "#     rmsse_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[2], header=True)\n",
        "#     ope_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[3], header=True)\n",
        "#     spl_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[4], header=True)"
      ],
      "metadata": {
        "id": "tA8yuf2E3AEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# PLOT\n",
        "########################\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Update the function to include filtering based on 'unique_id'\n",
        "def plot_data(unique_id):\n",
        "    # Define x_column and y_columns directly\n",
        "    x_column = data2use.columns[1]\n",
        "    y_columns = [data2use.columns[2], data2use.columns[3], data2use.columns[4], data2use.columns[5], data2use.columns[6], data2use.columns[7]]\n",
        "\n",
        "    # Filter data based on selected unique_id\n",
        "    filtered_data = data2use[data2use['unique_id'] == unique_id]\n",
        "\n",
        "    # Set up a 1x3 grid of subplots\n",
        "    fig, (ax1, ax4) = plt.subplots(1, 2, figsize=(25, 5), gridspec_kw={'width_ratios': [4, 1]}) # Adjust layout for table\n",
        "\n",
        "    # Plotting multiple y-axes on the first subplot\n",
        "    for y_column in y_columns:\n",
        "        ax1.plot(filtered_data[x_column], filtered_data[y_column], label=y_column)\n",
        "    ax1.set_xlabel(x_column)\n",
        "    ax1.set_ylabel('Values')\n",
        "    ax1.set_title(f'Revenue for {unique_id}')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Remove axis for table\n",
        "    ax4.axis('off')\n",
        "    ax4.axis('tight')\n",
        "\n",
        "    # Displaying the sum table\n",
        "    display_data = filtered_data[[x_column] + list(y_columns)].copy()\n",
        "    display_data = display_data[display_data['ds']>=fct_st_date]\n",
        "    display_data['ds'] = display_data['ds'].dt.strftime('%m/%d/%Y')\n",
        "\n",
        "    # Create a sum row\n",
        "    sum_values = {x_column: 'Sum'}\n",
        "    for col in list(y_columns):\n",
        "        sum_values[col] = display_data[col].sum()\n",
        "    sum_row = pd.DataFrame([sum_values])\n",
        "\n",
        "    # Create a % diff row\n",
        "    actuals_sum = sum_values['Actuals']\n",
        "    pdiff_values = {x_column: '% Diff'}\n",
        "    for col in list(y_columns):\n",
        "        pdiff_values[col] = ((display_data[col].sum()-actuals_sum) / actuals_sum) * 100 if actuals_sum != 0 else None\n",
        "        pdiff_values[col] = round(pdiff_values[col], 2)\n",
        "    perc_diff_row = pd.DataFrame([pdiff_values])\n",
        "\n",
        "    # Stack the sum row\n",
        "    display_data = pd.concat([sum_row, display_data], ignore_index=True)\n",
        "\n",
        "    # Round the values and add commas\n",
        "    for column in y_columns:\n",
        "        if column in display_data.columns:\n",
        "            # Round to two decimal places\n",
        "            display_data[column] = display_data[column].round(2)\n",
        "            # Format with commas\n",
        "            display_data[column] = display_data[column].apply(lambda x: f\"{x:,.2f}\")\n",
        "\n",
        "    # Stack the % diff and remove 'Actuals Train'\n",
        "    display_data = pd.concat([perc_diff_row, display_data], ignore_index=True)\n",
        "    display_data = display_data.drop('Actuals (Train)', axis=1)\n",
        "\n",
        "    # Convert perc_diff_data to array for table\n",
        "    table_data = display_data.to_numpy()\n",
        "    # Add table at the right\n",
        "    table = ax4.table(cellText=table_data, colLabels=display_data.columns, loc='right')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(8.5)  # Set smaller font size if necessary\n",
        "    table.scale(4, 1.8)  # Adjust scale to fit\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# data2use = ts2fix\n",
        "# data2use = tsnonspend\n",
        "data2use = data2plot\n",
        "# data2use = rev_at_hier2plot\n",
        "\n",
        "substring1 = 'jos'\n",
        "substring2 = 'jos'\n",
        "\n",
        "# Update data2use to include rows where 'unique_id' contains either substring1 or substring2\n",
        "data2use = data2use[data2use['unique_id'].str.contains(substring1 + '|' + substring2, case=False, na=False)]\n",
        "\n",
        "\n",
        "# Create widgets\n",
        "unique_id_selector = widgets.SelectionSlider(\n",
        "    options=data2use['unique_id'].unique(),\n",
        "    description='unique_id:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "# Display interactive plot\n",
        "interact(plot_data, unique_id=unique_id_selector)"
      ],
      "metadata": {
        "id": "orhsNKOrfL7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i1lK0A7UPSkqEp7I06G70oHrSC0UDtQy",
      "authorship_tag": "ABX9TyN85xg7wI7I2YemNLgzflVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}