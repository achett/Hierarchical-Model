{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achett/Hierarchical-Model/blob/main/Bayesian_Hierarchical_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WPJgqdPdNT1D",
        "outputId": "0058d480-51f2-4aad-d412-1e5501bbd029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hierarchicalforecast\n",
            "  Downloading hierarchicalforecast-0.4.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.24 (from hierarchicalforecast)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (0.58.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (1.2.2)\n",
            "Collecting quadprog (from hierarchicalforecast)\n",
            "  Downloading quadprog-0.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.2/508.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hierarchicalforecast) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->hierarchicalforecast) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->hierarchicalforecast) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hierarchicalforecast) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->hierarchicalforecast) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->hierarchicalforecast) (1.16.0)\n",
            "Installing collected packages: numpy, quadprog, hierarchicalforecast\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hierarchicalforecast-0.4.1 numpy-1.23.5 quadprog-0.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0cd0dfd733414cfbbcca64629e649e30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting statsforecast\n",
            "  Downloading statsforecast-1.7.3-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from statsforecast) (2.2.1)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from statsforecast) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from statsforecast) (4.66.2)\n",
            "Collecting fugue>=0.8.1 (from statsforecast)\n",
            "  Downloading fugue-0.8.7-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast>=0.0.24 (from statsforecast)\n",
            "  Downloading utilsforecast-0.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from statsforecast) (3.3.0)\n",
            "Collecting triad>=0.9.3 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading triad-0.9.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting qpd>=0.4.4 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading qpd-0.4.4-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugue-sql-antlr>=0.1.6 (from fugue>=0.8.1->statsforecast)\n",
            "  Downloading fugue-sql-antlr-0.2.0.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (20.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast) (3.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.0->statsforecast) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->statsforecast) (2023.4)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->statsforecast) (23.2)\n",
            "Collecting antlr4-python3-runtime<4.12 (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast)\n",
            "  Downloading antlr4_python3_runtime-4.11.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels>=0.13.2->statsforecast) (1.16.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast) (14.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast) (2023.6.0)\n",
            "Collecting fs (from triad>=0.9.3->fugue>=0.8.1->statsforecast)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->fugue>=0.8.1->statsforecast) (2.1.5)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast) (67.7.2)\n",
            "Building wheels for collected packages: fugue-sql-antlr\n",
            "  Building wheel for fugue-sql-antlr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fugue-sql-antlr: filename=fugue_sql_antlr-0.2.0-py3-none-any.whl size=158196 sha256=0bf00e323411c7247d0fc194692707b40b295cf95f9b38fbbbe3088a8ba41589\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/b5/4e/216953a1c711da55de29ed7ecf158b4a5bf32ef93d69ad66dd\n",
            "Successfully built fugue-sql-antlr\n",
            "Installing collected packages: antlr4-python3-runtime, fs, utilsforecast, triad, fugue-sql-antlr, adagio, qpd, fugue, statsforecast\n",
            "Successfully installed adagio-0.2.4 antlr4-python3-runtime-4.11.1 fs-2.4.16 fugue-0.8.7 fugue-sql-antlr-0.2.0 qpd-0.4.4 statsforecast-1.7.3 triad-0.9.5 utilsforecast-0.1.1\n",
            "Collecting datasetsforecast\n",
            "  Downloading datasetsforecast-0.0.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (3.9.3)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (0.8.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (4.66.2)\n",
            "Requirement already satisfied: xlrd>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasetsforecast) (2.0.1)\n",
            "Requirement already satisfied: triad>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.9.5)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (0.2.0)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (20.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->datasetsforecast) (3.1.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasetsforecast) (4.0.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->datasetsforecast) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasetsforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasetsforecast) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datasetsforecast) (2024.2.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->datasetsforecast) (4.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->datasetsforecast) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasetsforecast) (1.16.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (14.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (2023.6.0)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (2.4.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->fugue>=0.8.1->datasetsforecast) (2.1.5)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->datasetsforecast) (67.7.2)\n",
            "Installing collected packages: datasetsforecast\n",
            "Successfully installed datasetsforecast-0.0.8\n",
            "Collecting darts\n",
            "  Downloading darts-0.28.0-py3-none-any.whl (846 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.9/846.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from darts) (0.44)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from darts) (3.7.1)\n",
            "Collecting nfoursid>=1.0.0 (from darts)\n",
            "  Downloading nfoursid-1.0.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from darts) (1.23.5)\n",
            "Collecting pmdarima>=1.8.0 (from darts)\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyod>=0.9.5 (from darts)\n",
            "  Downloading pyod-1.1.3.tar.gz (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from darts) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from darts) (1.11.4)\n",
            "Collecting shap>=0.40.0 (from darts)\n",
            "  Downloading shap-0.45.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (538 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsforecast>=1.4 in /usr/local/lib/python3.10/dist-packages (from darts) (1.7.3)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from darts) (0.14.1)\n",
            "Collecting tbats>=1.1.0 (from darts)\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.10/dist-packages (from darts) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from darts) (4.10.0)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2023.7.0)\n",
            "Requirement already satisfied: xgboost>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.0.3)\n",
            "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=2.1 (from darts)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from darts) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from darts) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->darts) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->darts) (2023.4)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (3.0.9)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->darts) (67.7.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from pyod>=0.9.5->darts) (0.58.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.9.5->darts) (1.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.5.0->darts) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities>=0.8.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->darts) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->darts) (3.3.0)\n",
            "Collecting slicer==0.0.7 (from shap>=0.40.0->darts)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.40.0->darts) (2.2.1)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.8.7)\n",
            "Requirement already satisfied: utilsforecast>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=1.4->darts) (0.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.0->darts) (0.5.6)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.1->darts) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->darts) (2.1.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.9.3)\n",
            "Requirement already satisfied: triad>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.9.5)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.0)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (20.11.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->darts) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->darts) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (4.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=1.4->darts) (4.11.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (14.0.2)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (2.4.16)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.1.3-py3-none-any.whl size=190251 sha256=7e8f5ce893b1b6396a26f6f45d9ef076a45d8effb33ec0154de01118beae2995\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/f8/db/124d43bec122d6ec0ab3713fadfe25ebed8af52ec561682b4e\n",
            "Successfully built pyod\n",
            "Installing collected packages: tensorboardX, slicer, lightning-utilities, torchmetrics, shap, pyod, nfoursid, pytorch-lightning, pmdarima, tbats, darts\n",
            "Successfully installed darts-0.28.0 lightning-utilities-0.10.1 nfoursid-1.0.1 pmdarima-2.0.4 pyod-1.1.3 pytorch-lightning-2.2.1 shap-0.45.0 slicer-0.0.7 tbats-1.1.3 tensorboardX-2.6.2.2 torchmetrics-1.3.1\n",
            "Collecting mlforecast\n",
            "  Downloading mlforecast-0.12.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlforecast) (2.2.1)\n",
            "Collecting coreforecast>=0.0.7 (from mlforecast)\n",
            "  Downloading coreforecast-0.0.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.5/193.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from mlforecast) (2023.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from mlforecast) (0.58.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlforecast) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mlforecast) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mlforecast) (1.2.2)\n",
            "Requirement already satisfied: utilsforecast>=0.0.27 in /usr/local/lib/python3.10/dist-packages (from mlforecast) (0.1.1)\n",
            "Collecting window-ops (from mlforecast)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from coreforecast>=0.0.7->mlforecast) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mlforecast) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mlforecast) (2023.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->mlforecast) (0.41.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mlforecast) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->mlforecast) (1.16.0)\n",
            "Installing collected packages: coreforecast, window-ops, mlforecast\n",
            "Successfully installed coreforecast-0.0.7 mlforecast-0.12.0 window-ops-0.0.15\n"
          ]
        }
      ],
      "source": [
        "!pip install hierarchicalforecast\n",
        "!pip install statsforecast\n",
        "!pip install datasetsforecast\n",
        "!pip install nixtlats>=0.1.0\n",
        "!pip install darts\n",
        "!pip install mlforecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uUqzb8wbrU_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8681d42-00d4-4ff7-dd2c-a265d2cc6c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "# PACKAGES\n",
        "########################\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import openpyxl\n",
        "from datetime import datetime\n",
        "\n",
        "from statsforecast.core import StatsForecast\n",
        "from statsforecast.models import AutoARIMA, Naive, AutoETS, AutoCES, AutoTheta\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from hierarchicalforecast.core import HierarchicalReconciliation\n",
        "from hierarchicalforecast.evaluation import HierarchicalEvaluation\n",
        "from hierarchicalforecast.methods import BottomUp, TopDown, MiddleOut, MinTrace, OptimalCombination, ERM, PERMBU, Bootstrap, Normality\n",
        "from hierarchicalforecast.utils import aggregate\n",
        "from nixtlats import TimeGPT\n",
        "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
        "\n",
        "\n",
        "from darts import TimeSeries, concatenate\n",
        "from darts.models import RegressionModel, LightGBMModel, ExponentialSmoothing, StatsForecastAutoETS, StatsForecastAutoARIMA, KalmanForecaster\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from lightgbm import LGBMRegressor\n",
        "from darts.metrics import mae, rmse, mape, quantile_loss, mse, ope\n",
        "from darts.utils.likelihood_models import QuantileRegression\n",
        "\n",
        "pd.options.display.float_format = '{:,.2f}'.format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6iyKhHrjGb0t"
      },
      "outputs": [],
      "source": [
        "##############\n",
        "# PARAMS\n",
        "##############\n",
        "fct_periods = 12\n",
        "fct_st_date = '2023-04-01'\n",
        "fct_end_date = '2023-12-01'\n",
        "\n",
        "# Create hierarchical structure and constraints\n",
        "hierarchy_levels = [['top_level'],\n",
        "                    ['top_level', 'level2'],\n",
        "                    ['top_level', 'level2', 'level3'],\n",
        "                    ['top_level', 'level2', 'level3', 'bottom_level']]\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/regional_hierarchy.xlsx'\n",
        "sheet_name = 'regional_hierarchy v2'\n",
        "r_hier = pd.read_excel(inputFile, sheet_name=sheet_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "# FUNCTIONS\n",
        "##############\n",
        "\n",
        "def prepare_data(data, r_hier):\n",
        "    # Merge hierarchy\n",
        "    data = data.merge(r_hier, how='left', left_on='cost_object', right_on='cost_object')\n",
        "\n",
        "    # Transform date and y\n",
        "    data['ds'] = pd.to_datetime(data['ds'])\n",
        "    data['y'] = data['y'].astype(float)\n",
        "\n",
        "    # Address NA values\n",
        "    data['volume'] = data['y'].fillna(0)\n",
        "    data['region'] = data['region'].fillna('')\n",
        "    data['cost_object'] = data['cost_object'].fillna('')\n",
        "    data['product'] = data['product'].fillna('')\n",
        "\n",
        "    # Create hierarchical dataframe\n",
        "    data['top_level'] = 'global'  # Assuming 'top_level' does not contain '/', otherwise add a replace line for it too.\n",
        "    data.rename(columns={'product': 'level2', 'region': 'level3', 'cost_object': 'bottom_level'}, inplace=True)\n",
        "    data = data[['level2', 'level3', 'bottom_level', 'top_level', 'ds', 'y']]\n",
        "\n",
        "    # Replace '/' with '_' in the four columns\n",
        "    data['level2'] = data['level2'].str.replace('/', '_')\n",
        "    data['level3'] = data['level3'].str.replace('/', '_')\n",
        "    data['bottom_level'] = data['bottom_level'].str.replace('/', '_')\n",
        "\n",
        "    data['unique_id'] = data['top_level'] + '/' + data['level2'] + '/' + data['level3'] + '/' + data['bottom_level']\n",
        "\n",
        "    return data\n",
        "\n",
        "def prepare_feature(data, r_hier, volume_act2, feature_name):\n",
        "\n",
        "    # Select and rename columns\n",
        "    data = data[['cost_object', 'product', 'ds', feature_name]].rename(columns={feature_name: 'y'})\n",
        "\n",
        "    # Apply any additional preparation (assuming prepare_data is a function you have defined)\n",
        "    data = prepare_data(data, r_hier)\n",
        "\n",
        "    # Rename the columns back\n",
        "    data = data.rename(columns={'y': feature_name})\n",
        "\n",
        "    # Merge with the volume_act2 dataframe\n",
        "    merged_df = data.merge(volume_act2[['unique_id', 'ds']], how='right', on=['unique_id', 'ds'])\n",
        "\n",
        "    return merged_df\n"
      ],
      "metadata": {
        "id": "-0tzZ-xnIBTt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "# DATA LOAD\n",
        "##############\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/budgetFY23.csv'\n",
        "budget = pd.read_csv(inputFile)\n",
        "# budget = budget[budget['category']=='EQUIV_UNIT - Equivalent Units']\n",
        "budget = budget[budget['category']=='UC110000 - Total Revenue']\n",
        "budget.rename(columns={'country': 'cost_object'}, inplace=True)\n",
        "budget = prepare_data(budget, r_hier)\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/revenue_output.csv'\n",
        "volume_act = pd.read_csv(inputFile)\n",
        "volume_act.rename(columns={'value': 'y'}, inplace=True)\n",
        "volume_act = prepare_data(volume_act, r_hier)\n",
        "\n",
        "inputFile = '/content/drive/MyDrive/Colab Notebooks/SGA Prediction/data/sga_output.csv'\n",
        "sga = pd.read_csv(inputFile)\n",
        "\n",
        "sga1 = prepare_feature(sga, r_hier, volume_act, 'AP')\n",
        "sga2 = prepare_feature(sga, r_hier, volume_act, 'Field_Sales')"
      ],
      "metadata": {
        "id": "u3E-oM38IFVP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_hier.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6R43hTpe4V47",
        "outputId": "c035207e-ac55-44f9-cf95-a5b0e355eb57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          region               cost_object  Unnamed: 2\n",
              "0  Greater China  D_CN_TOTAL - China Total       97.00\n",
              "1          Japan     D_E_AFLIC - Licensing       41.00\n",
              "2    East Europe     D_E_AUSTRIA - Austria       11.00\n",
              "3        BENELUX     D_E_BELGIUM - Belgium        8.00\n",
              "4    East Europe         D_E_BG - Bulgaria       22.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06297458-b607-47ee-947b-7395d6af152a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>region</th>\n",
              "      <th>cost_object</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Greater China</td>\n",
              "      <td>D_CN_TOTAL - China Total</td>\n",
              "      <td>97.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>D_E_AFLIC - Licensing</td>\n",
              "      <td>41.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>East Europe</td>\n",
              "      <td>D_E_AUSTRIA - Austria</td>\n",
              "      <td>11.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BENELUX</td>\n",
              "      <td>D_E_BELGIUM - Belgium</td>\n",
              "      <td>8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>East Europe</td>\n",
              "      <td>D_E_BG - Bulgaria</td>\n",
              "      <td>22.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06297458-b607-47ee-947b-7395d6af152a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06297458-b607-47ee-947b-7395d6af152a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06297458-b607-47ee-947b-7395d6af152a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3960668-d263-4277-955c-26fac425ab7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3960668-d263-4277-955c-26fac425ab7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3960668-d263-4277-955c-26fac425ab7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "r_hier",
              "summary": "{\n  \"name\": \"r_hier\",\n  \"rows\": 97,\n  \"fields\": [\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"SESA\",\n          \"LATAM\",\n          \"Greater China\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cost_object\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 97,\n        \"samples\": [\n          \"D_I_JORDAN - Jordan\",\n          \"D_I_ALGERIA - Algeria\",\n          \"US10 - Astellas Pharma US, Inc.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Unnamed: 2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.318232336966872,\n        \"min\": 2.0,\n        \"max\": 103.0,\n        \"num_unique_values\": 86,\n        \"samples\": [\n          53.0,\n          97.0,\n          58.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# SAMPLE\n",
        "########################\n",
        "# Subset\n",
        "regs2include = ['global/ENZA - Enzalutamide/North America/US10 - Astellas Pharma US, Inc.', 'global/REGADENOSN - Regadenoson/North America/US10 - Astellas Pharma US, Inc.']\n",
        "volume_act = volume_act[volume_act['unique_id'].isin(regs2include)]\n",
        "\n",
        "# level2include = ['CIS']\n",
        "# volume_act = volume_act[volume_act['level3'].isin(level2include)]"
      ],
      "metadata": {
        "id": "vvMW8Aj7jmRz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# IDENTIFY UNIVERSE\n",
        "########################\n",
        "tested_ts = set(budget['unique_id'].unique()).intersection(volume_act['unique_id'].unique())\n",
        "\n",
        "# Find unique IDs present in budget_h but not in rev\n",
        "unique_ids_in_budget_not_in_rev = set(budget['unique_id'].unique()).difference(volume_act['unique_id'].unique())\n",
        "\n",
        "# Find unique IDs present in rev but not in budget_h\n",
        "unique_ids_in_rev_not_in_budget = set(volume_act['unique_id'].unique()).difference(budget['unique_id'].unique())\n",
        "\n",
        "# Filter volume\n",
        "volume_act = volume_act[volume_act['unique_id'].isin(tested_ts)]"
      ],
      "metadata": {
        "id": "JTjLsBc6ATQQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# SEGMENT TIME SERIES\n",
        "########################\n",
        "new_products = ['ENFORTUMAB - Enforumab Vedotin', 'ROXADUSTNT - Roxadustant']\n",
        "loe_products = ['REGADENOSN - Regadenoson']\n",
        "div_products = ['MICAFUNGIN - Micafungin Sodium']\n",
        "\n",
        "new_ids = volume_act[volume_act['level2'].isin(new_products)]['unique_id'].unique().tolist()\n",
        "loe_ids = volume_act[volume_act['level2'].isin(loe_products)]['unique_id'].unique().tolist()\n",
        "divested_ids = volume_act[volume_act['level2'].isin(div_products)]['unique_id'].unique().tolist()\n",
        "\n",
        "# IDs with A&P and Field Sales Spend\n",
        "grouped1 = sga1.groupby('unique_id')[['AP']].sum()\n",
        "grouped2 = sga2.groupby('unique_id')[['Field_Sales']].sum()\n",
        "spend_ids = set(grouped1[(grouped1['AP'] > 0)].index.tolist() + grouped2[(grouped2['Field_Sales'] > 0)].index.tolist())\n",
        "spend_ids = spend_ids.difference(new_ids + loe_ids + divested_ids)\n",
        "\n",
        "# IDs with no spend\n",
        "non_spend_ids = volume_act[~volume_act['unique_id'].isin(spend_ids)]['unique_id'].unique()\n",
        "\n",
        "arima_regions = ['Japan', 'BENELUX', 'North America', 'Nordic', 'Baltic']\n",
        "ets_regions = ['East Europe', 'West Europe', 'Greater China', 'MEA', 'LATAM', 'SESA', 'CIS']\n",
        "# arima_ids = volume_act[(volume_act['level3'].isin(arima_regions)) & (~volume_act['unique_id'].isin(spend_ids))]['unique_id'].unique().tolist()\n",
        "# ets_ids = volume_act[(volume_act['level3'].isin(ets_regions)) & (~volume_act['unique_id'].isin(spend_ids))]['unique_id'].unique().tolist()\n",
        "\n",
        "arima_ids = volume_act[(volume_act['level3'].isin(arima_regions))]['unique_id'].unique().tolist()\n",
        "ets_ids = volume_act[(volume_act['level3'].isin(ets_regions))]['unique_id'].unique().tolist()"
      ],
      "metadata": {
        "id": "Y7_vce08-o24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ########################\n",
        "# # INTERMITTENT DEMAND CANDIDATES\n",
        "# ########################\n",
        "\n",
        "# # Function to calculate the percentage of zeros after the first non-zero\n",
        "# def calculate_percentage_zeros(df):\n",
        "#     # Find the index of the first non-zero entry in 'y'\n",
        "#     first_non_zero_index = df.loc[df['y'] != 0].index.min()\n",
        "#     # If there are no non-zero values, return None or 0 based on your preference\n",
        "#     if pd.isna(first_non_zero_index):\n",
        "#         return None  # Or return 0 if you want to treat this as 0% zeros following non-zero\n",
        "#     # Select the subset of 'y' after the first non-zero\n",
        "#     post_non_zero_series = df.loc[first_non_zero_index:, 'y']\n",
        "#     # Count the number of zeros in this subset\n",
        "#     num_zeros = (post_non_zero_series == 0).sum()\n",
        "#     # Calculate the percentage of zeros\n",
        "#     percentage_zeros = num_zeros / len(post_non_zero_series) * 100\n",
        "#     return percentage_zeros\n",
        "\n",
        "# # Apply the function to each group and reset index to make unique_id a column\n",
        "# percentage_zeros_df = volume_act.groupby('unique_id').apply(calculate_percentage_zeros).reset_index(name='percentage_zeros')\n",
        "\n",
        "# inter_demand_ids = percentage_zeros_df[percentage_zeros_df['percentage_zeros']>=50]['unique_id'].tolist()\n"
      ],
      "metadata": {
        "id": "Aw-07I-33MIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RUN ETS & ARIMA\n",
        "########################\n",
        "def convert_fct2df(forecasts):\n",
        "    forecast_dfs = []\n",
        "    for unique_id, forecast_ts in forecasts.items():\n",
        "        df = TimeSeries.quantiles_df(forecast_ts, quantiles=[0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995])\n",
        "        df['unique_id'] = unique_id\n",
        "        df = df.reset_index()\n",
        "        df = df.rename(columns={'y_0.5': 'y'})\n",
        "        forecast_dfs.append(df)\n",
        "\n",
        "    # Concatenate all forecast DataFrames into a single DataFrame\n",
        "    all_forecasts_df = pd.concat(forecast_dfs, axis=0)\n",
        "\n",
        "    # Reorder and rename columns as needed\n",
        "    columns = ['unique_id'] + [col for col in all_forecasts_df.columns if col != 'unique_id']\n",
        "    all_forecasts_df = all_forecasts_df[columns]\n",
        "\n",
        "    all_forecasts_df.columns.name = None\n",
        "\n",
        "    return all_forecasts_df\n",
        "\n",
        "def generate_time_series_dict(data, fct_periods, filter_data):\n",
        "    # Split train/test sets\n",
        "    test = data.groupby('unique_id').tail(fct_periods)\n",
        "    train = data.drop(test.index)\n",
        "\n",
        "    # Prepare time series dataframes\n",
        "    time_series_dfs = {uid: group for uid, group in train.groupby('unique_id')}\n",
        "    time_series_dict = {}\n",
        "\n",
        "    if filter_data:\n",
        "        # Filter out time series with insufficient non-zero data points\n",
        "        filtered_time_series_dfs = {}\n",
        "        for uid, group in time_series_dfs.items():\n",
        "            non_zero_index = group['y'].ne(0).idxmax()\n",
        "            start_index = max(0, non_zero_index - (13 - 1))\n",
        "            filtered_df = group.loc[non_zero_index:] if group.loc[non_zero_index:].shape[0] >= 13 else group.loc[start_index:]\n",
        "            if not filtered_df.empty:\n",
        "                filtered_time_series_dfs[uid] = filtered_df\n",
        "        # Convert each filtered DataFrame into a Darts TimeSeries object\n",
        "        time_series_dict = {uid: TimeSeries.from_dataframe(group, 'ds', 'y') for uid, group in filtered_time_series_dfs.items()}\n",
        "    else:\n",
        "        # Convert each original DataFrame into a Darts TimeSeries object without filtering\n",
        "        time_series_dict = {uid: TimeSeries.from_dataframe(group, 'ds', 'y') for uid, group in time_series_dfs.items()}\n",
        "\n",
        "    return time_series_dict\n",
        "\n",
        "def generate_forecast(data, fct_periods, model2use, filter_data=True):\n",
        "\n",
        "    # Use the nested function to generate the time series dictionary\n",
        "    time_series_dict = generate_time_series_dict(data, fct_periods, filter_data)\n",
        "\n",
        "    # Create and fit a model for each time series\n",
        "    models = {}\n",
        "    for uid, series in time_series_dict.items():\n",
        "        model = get_model(model2use)\n",
        "        model.fit(series)\n",
        "        models[uid] = model\n",
        "\n",
        "    # Forecasting\n",
        "    fct_dict = {uid: model.predict(fct_periods, num_samples=20) for uid, model in models.items()}\n",
        "    # Convert forecasts into a dataframe\n",
        "    fct_df = convert_fct2df(fct_dict)\n",
        "\n",
        "    return fct_dict, fct_df\n",
        "\n",
        "# Function to dynamically get the model instance\n",
        "def get_model(model_name):\n",
        "    if model_name == 'AutoETS':\n",
        "        return StatsForecastAutoETS()\n",
        "    elif model_name == 'ARIMA':\n",
        "        return StatsForecastAutoARIMA(season_length=12)\n",
        "    elif model_name == 'KF':\n",
        "        return KalmanForecaster(dim_x=12)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "ets_dict, ets_df = generate_forecast(volume_act, fct_periods, model2use='AutoETS', filter_data=True)\n",
        "arima_dict, arima_df = generate_forecast(volume_act, fct_periods, model2use='ARIMA', filter_data=True)\n",
        "# kf_fct = generate_forecast(volume_act, fct_periods, model2use='KF', filter_data=True)\n"
      ],
      "metadata": {
        "id": "hHFkuRx-53nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volume_act['unique_id'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfG7qRa0dUZ1",
        "outputId": "d67d6b26-0a0a-4db5-fe40-c37925ab47f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "603"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RUN QUARTERLY MODEL\n",
        "########################\n",
        "# Function to resample and sum data by quarter for each group\n",
        "def resample_group(group):\n",
        "    group = group.set_index('ds')  # Set 'ds' as the index\n",
        "    resampled_group = group.resample('Q').agg({'y': 'sum'})  # Aggregate data by quarter\n",
        "    return resampled_group\n",
        "\n",
        "quarterly_data = volume_act\n",
        "quarterly_data['ds'] = pd.to_datetime(quarterly_data['ds'])\n",
        "\n",
        "# Group by 'unique_id' and apply the resampling function\n",
        "quarterly_data = quarterly_data.groupby('unique_id').apply(resample_group).reset_index()\n",
        "\n"
      ],
      "metadata": {
        "id": "PYEIfAua_8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# XTREND - DECAY\n",
        "########################\n",
        "def apply_exponential_decay(df, start_date, end_date, end_value_percentage, target_unique_ids):\n",
        "    # Convert 'ds' to datetime if it's not already\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    start_date = pd.to_datetime(start_date)\n",
        "    end_date = pd.to_datetime(end_date)\n",
        "\n",
        "    # Function to apply decay for each group\n",
        "    def decay_group(group):\n",
        "        # Only apply changes if unique_id is in target_unique_ids\n",
        "        if group['unique_id'].iloc[0] not in target_unique_ids:\n",
        "            return group\n",
        "\n",
        "        # Sort by date to ensure proper indexing\n",
        "        group = group.sort_values(by='ds')\n",
        "\n",
        "        # Columns to apply decay to\n",
        "        decay_columns = [col for col in group.columns if col not in ['unique_id', 'ds']]\n",
        "\n",
        "        # Initialize a dictionary to keep the end values for each decay column\n",
        "        end_values = {}\n",
        "\n",
        "        # Find start and end values and dates for each column\n",
        "        for col in decay_columns:\n",
        "            if start_date in group['ds'].values and end_date in group['ds'].values:\n",
        "                start_value = group.loc[group['ds'] == start_date, col].iloc[0]\n",
        "                end_value = start_value * end_value_percentage\n",
        "                end_values[col] = end_value  # Store the end value for this column\n",
        "\n",
        "                # Calculate the decay rate based on exponential decay formula\n",
        "                days = (end_date - start_date).days\n",
        "                decay_rate = np.log(end_value / start_value) / days\n",
        "\n",
        "                # Apply exponential decay for dates between start_date and end_date\n",
        "                for date in pd.date_range(start_date, end_date):\n",
        "                    if date in group['ds'].values:\n",
        "                        t = (date - start_date).days\n",
        "                        new_value = start_value * np.exp(decay_rate * t)\n",
        "                        group.loc[group['ds'] == date, col] = new_value\n",
        "\n",
        "        # Replace column values for dates after end_date with the respective end values\n",
        "        for col, end_value in end_values.items():\n",
        "            if end_value is not None:  # Ensure there was an end value calculated\n",
        "                group.loc[group['ds'] > end_date, col] = end_value\n",
        "\n",
        "        return group\n",
        "\n",
        "    # Apply the decay_group function to each group and return the modified dataframe\n",
        "    return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n",
        "\n",
        "# Apply exponential decay\n",
        "# lgbm_fct.rename(columns={'LGBM': 'y'}, inplace=True)\n",
        "ets_df.rename(columns={'ETS': 'y'}, inplace=True)\n",
        "arima_df.rename(columns={'ARIMA': 'y'}, inplace=True)\n",
        "\n",
        "# Micafungin\n",
        "arima_df = apply_exponential_decay(arima_df, '2023-07-01', '2023-08-01', 0, divested_ids)\n",
        "ets_df = apply_exponential_decay(ets_df, '2023-07-01', '2023-08-01', 0, divested_ids)\n",
        "\n",
        "# Lexiscan\n",
        "arima_df = apply_exponential_decay(arima_df, '2023-04-01', '2023-12-01', .1, loe_ids)\n",
        "ets_df = apply_exponential_decay(ets_df, '2023-04-01', '2023-12-01', .1, loe_ids)\n"
      ],
      "metadata": {
        "id": "y-UfqmzMxgIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d81e40-02e9-4bd0-c2e3-4c62cc0a8268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7d582f2577b1>:34: RuntimeWarning: divide by zero encountered in log\n",
            "  decay_rate = np.log(end_value / start_value) / days\n",
            "<ipython-input-13-7d582f2577b1>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  new_value = start_value * np.exp(decay_rate * t)\n",
            "<ipython-input-13-7d582f2577b1>:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n",
            "<ipython-input-13-7d582f2577b1>:34: RuntimeWarning: divide by zero encountered in log\n",
            "  decay_rate = np.log(end_value / start_value) / days\n",
            "<ipython-input-13-7d582f2577b1>:40: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  new_value = start_value * np.exp(decay_rate * t)\n",
            "<ipython-input-13-7d582f2577b1>:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n",
            "<ipython-input-13-7d582f2577b1>:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n",
            "<ipython-input-13-7d582f2577b1>:51: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  return df.groupby('unique_id').apply(decay_group).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# XTREND - GROWTH\n",
        "########################\n",
        "# volume_act = volume_act[~volume_act['unique_id'].isin(new_ids)]"
      ],
      "metadata": {
        "id": "N3pMlJne3M5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAbo6hYuCf3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98357a5a-d661-4213-fce4-6d7b6c4f5c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-64a0af311e5d>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  volume_act_xsm.rename(columns={'y': 'Actuals'}, inplace=True)\n",
            "<ipython-input-15-64a0af311e5d>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  budget2.rename(columns={'y': 'Budget'}, inplace=True)\n",
            "<ipython-input-15-64a0af311e5d>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ets_df2.rename(columns={'y': 'ETS'}, inplace=True)\n",
            "<ipython-input-15-64a0af311e5d>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  arima_df2.rename(columns={'y': 'ARIMA'}, inplace=True)\n",
            "<ipython-input-15-64a0af311e5d>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data4metrics['lowest_diff_col'] = data4metrics['unique_id'].map(min_diff_col)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lowest_diff_col\n",
              "Budget         385\n",
              "SelectedFCT    218\n",
              "Name: unique_id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "########################\n",
        "# METRICS\n",
        "########################\n",
        "# Subset\n",
        "volume_act_xsm = volume_act[['unique_id', 'ds', 'y']]\n",
        "budget2 = budget[['unique_id', 'ds', 'y']]\n",
        "ets_df2 = ets_df[['unique_id', 'ds', 'y']]\n",
        "arima_df2 = arima_df[['unique_id', 'ds', 'y']]\n",
        "\n",
        "# Assign names\n",
        "volume_act_xsm.rename(columns={'y': 'Actuals'}, inplace=True)\n",
        "budget2.rename(columns={'y': 'Budget'}, inplace=True)\n",
        "ets_df2.rename(columns={'y': 'ETS'}, inplace=True)\n",
        "arima_df2.rename(columns={'y': 'ARIMA'}, inplace=True)\n",
        "\n",
        "# Merge actuals, budget and forecast\n",
        "rev_at = volume_act_xsm.merge(ets_df2, on=['unique_id', 'ds'], how='left')\n",
        "rev_at = rev_at.merge(budget2, on=['unique_id', 'ds'], how='left')\n",
        "rev_at = rev_at.merge(arima_df2, on=['unique_id', 'ds'], how='left')\n",
        "\n",
        "# Conditions for selection\n",
        "# conditions = [rev_at['unique_id'].isin(spend_ids),rev_at['unique_id'].isin(arima_ids),rev_at['unique_id'].isin(ets_ids)]\n",
        "# choices = [rev_at['ARIMA'], rev_at['ARIMA'],rev_at['ETS']]\n",
        "\n",
        "conditions = [rev_at['unique_id'].isin(arima_ids),rev_at['unique_id'].isin(ets_ids)]\n",
        "choices = [rev_at['ARIMA'],rev_at['ETS']]\n",
        "\n",
        "# Creating the new column 'SelectedFCT' based on the conditions\n",
        "rev_at['SelectedFCT'] = np.select(conditions, choices, default=np.nan)\n",
        "\n",
        "# Only keep tested ts\n",
        "rev_at = rev_at[rev_at['unique_id'].isin(tested_ts)]\n",
        "\n",
        "# Filter for dates\n",
        "data4metrics = rev_at[(rev_at['ds']<=fct_end_date) & (rev_at['ds']>=fct_st_date)]\n",
        "\n",
        "# Sum up the values for each unique_id\n",
        "numeric_cols = data4metrics.columns.drop(['unique_id', 'ds'])\n",
        "summed_df = data4metrics.groupby('unique_id')[numeric_cols].sum()\n",
        "\n",
        "# Calculate difference and percentage differences from 'Actuals'\n",
        "absolute_diff = summed_df.subtract(summed_df['Actuals'], axis=0).abs()\n",
        "percentage_diff = summed_df.subtract(summed_df['Actuals'], axis=0).div(summed_df['Actuals'], axis=0).abs()\n",
        "\n",
        "# Drop the 'Actuals' column as we don't need to compare it with itself\n",
        "absolute_diff.drop(columns=['Actuals', 'ARIMA', 'ETS'], inplace=True)\n",
        "\n",
        "# Find the column with the lowest difference for each unique_id and add to metrics table\n",
        "min_diff_col = absolute_diff.idxmin(axis=1)\n",
        "data4metrics['lowest_diff_col'] = data4metrics['unique_id'].map(min_diff_col)\n",
        "\n",
        "# Find winner\n",
        "winner = data4metrics.groupby('lowest_diff_col')\n",
        "\n",
        "# Get Budget winners\n",
        "bud_winners = winner.get_group('Budget')['unique_id'].unique()\n",
        "\n",
        "winner['unique_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUFeFJhSRob_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "7332901a-19d1-4fb6-9eec-fed207248351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           unique_id         ds  Actuals  ETS  \\\n",
              "0  global/ENZA - Enzalutamide/Greater China/D_CN_... 2014-04-01      NaN  NaN   \n",
              "1  global/ENZA - Enzalutamide/Greater China/D_CN_... 2014-05-01      NaN  NaN   \n",
              "2  global/ENZA - Enzalutamide/Greater China/D_CN_... 2014-06-01      NaN  NaN   \n",
              "3  global/ENZA - Enzalutamide/Greater China/D_CN_... 2014-07-01      NaN  NaN   \n",
              "4  global/ENZA - Enzalutamide/Greater China/D_CN_... 2014-08-01      NaN  NaN   \n",
              "\n",
              "   Budget  ARIMA  SelectedFCT  Actuals (Train)  \n",
              "0     NaN    NaN          NaN             0.00  \n",
              "1     NaN    NaN          NaN             0.00  \n",
              "2     NaN    NaN          NaN             0.00  \n",
              "3     NaN    NaN          NaN             0.00  \n",
              "4     NaN    NaN          NaN             0.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6802c58-19bf-4d1d-8394-5d7a508b4af6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_id</th>\n",
              "      <th>ds</th>\n",
              "      <th>Actuals</th>\n",
              "      <th>ETS</th>\n",
              "      <th>Budget</th>\n",
              "      <th>ARIMA</th>\n",
              "      <th>SelectedFCT</th>\n",
              "      <th>Actuals (Train)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>global/ENZA - Enzalutamide/Greater China/D_CN_...</td>\n",
              "      <td>2014-04-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>global/ENZA - Enzalutamide/Greater China/D_CN_...</td>\n",
              "      <td>2014-05-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>global/ENZA - Enzalutamide/Greater China/D_CN_...</td>\n",
              "      <td>2014-06-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>global/ENZA - Enzalutamide/Greater China/D_CN_...</td>\n",
              "      <td>2014-07-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>global/ENZA - Enzalutamide/Greater China/D_CN_...</td>\n",
              "      <td>2014-08-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6802c58-19bf-4d1d-8394-5d7a508b4af6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6802c58-19bf-4d1d-8394-5d7a508b4af6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6802c58-19bf-4d1d-8394-5d7a508b4af6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-78f59e67-fb4a-424d-ae80-fdd3eed98b3c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78f59e67-fb4a-424d-ae80-fdd3eed98b3c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-78f59e67-fb4a-424d-ae80-fdd3eed98b3c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data2plot",
              "summary": "{\n  \"name\": \"data2plot\",\n  \"rows\": 70555,\n  \"fields\": [\n    {\n      \"column\": \"unique_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 603,\n        \"samples\": [\n          \"global/TACRO_XL - Tacrolimus Extended-Release/West Europe/D_E_FRANCE - France\",\n          \"global/TACRO_XL - Tacrolimus Extended-Release/MEA/D_I_TURKEY - Turkey\",\n          \"global/INFLUENZA - INFLUENZA/Japan/JP10 - Astellas Pharma Inc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ds\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-04-01 00:00:00\",\n        \"max\": \"2023-12-01 00:00:00\",\n        \"num_unique_values\": 117,\n        \"samples\": [\n          \"2017-12-01 00:00:00\",\n          \"2014-08-01 00:00:00\",\n          \"2018-09-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actuals\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1255081945.714266,\n        \"min\": -2343609724.0,\n        \"max\": 33862556533.750004,\n        \"num_unique_values\": 4630,\n        \"samples\": [\n          181343895.3556331,\n          4228860.0535277,\n          4763055.351237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ETS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1180556223.6999378,\n        \"min\": -896260745.3068018,\n        \"max\": 29611621866.657387,\n        \"num_unique_values\": 7015,\n        \"samples\": [\n          850209575.8037829,\n          109379908.8878191,\n          17274742.917918377\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Budget\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1381083464.9743252,\n        \"min\": -3219141999.99992,\n        \"max\": 31554032510.32051,\n        \"num_unique_values\": 3702,\n        \"samples\": [\n          8382875.5586131,\n          2712706.0339873,\n          10428503.0375677\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ARIMA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1172579501.9048855,\n        \"min\": -3196959201.4941945,\n        \"max\": 28672139031.970734,\n        \"num_unique_values\": 6703,\n        \"samples\": [\n          681041142.3690598,\n          -208606.97587401196,\n          6104163.725086401\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SelectedFCT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1175838335.8999004,\n        \"min\": -3196959201.4941945,\n        \"max\": 28672139031.970734,\n        \"num_unique_values\": 6955,\n        \"samples\": [\n          7333112.27026294,\n          997065.8454241357,\n          19255127.044286497\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actuals (Train)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 842196450.6654226,\n        \"min\": -3219140849.0,\n        \"max\": 30234012610.2,\n        \"num_unique_values\": 37387,\n        \"samples\": [\n          14127072.7,\n          117173305.23,\n          22986371.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "########################\n",
        "# CREATE PLOT DATA\n",
        "########################\n",
        "fct_st_date = pd.to_datetime(fct_st_date)\n",
        "\n",
        "# Add revenue actuals\n",
        "data2plot = rev_at.copy()\n",
        "data2plot['ds'] = pd.to_datetime(data2plot['ds'])\n",
        "\n",
        "# Update Actuals columns\n",
        "data2plot['Actuals (Train)'] = data2plot['Actuals'].copy()\n",
        "data2plot['Actuals'] = data2plot.apply(lambda row: row['Actuals'] if row['ds'] >= fct_st_date else None, axis=1)\n",
        "data2plot['Actuals (Train)'] = data2plot.apply(lambda row: row['Actuals (Train)'] if row['ds'] < fct_st_date else None, axis=1)\n",
        "\n",
        "# Filter to end date\n",
        "data2plot = data2plot[data2plot['ds']<=fct_end_date]\n",
        "\n",
        "# Find TS to fix\n",
        "ts2fix = data2plot[data2plot['unique_id'].isin(bud_winners)]\n",
        "tsnonspend = data2plot[data2plot['unique_id'].isin(non_spend_ids)]\n",
        "\n",
        "data2plot.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# CREATE HIERARCHICAL DATAFRAMES\n",
        "########################\n",
        "def split_unique_id_into_columns(df, column_name):\n",
        "\n",
        "    # Split 'unique_id' into 4 new columns\n",
        "    split_columns = df[column_name].str.split('/', expand=True)\n",
        "    # Rename the columns\n",
        "    split_columns.columns = ['top_level', 'level2', 'level3', 'bottom_level']\n",
        "    # Join back to original dataframe\n",
        "    result_df = pd.concat([df, split_columns], axis=1)\n",
        "\n",
        "    # Check for columns that are not in split_columns, 'unique_id' or 'ds'\n",
        "    for col in result_df.columns:\n",
        "        if col not in ['top_level', 'level2', 'level3', 'bottom_level', 'unique_id', 'ds']:\n",
        "            # Rename the column to 'y'\n",
        "            result_df = result_df.rename(columns={col: 'y'})\n",
        "            break  # Assuming only one column needs to be renamed\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def create_hier(data, hierarchy_levels, tested_ts, fct_periods):\n",
        "\n",
        "    # Filter data based on tested_ts\n",
        "    data_filtered = data[data['unique_id'].isin(tested_ts)]\n",
        "\n",
        "    # Identify the columns to aggregate\n",
        "    columns_to_aggregate = [col for col in data_filtered.columns if col not in ['unique_id', 'ds','top_level', 'level2', 'level3', 'bottom_level']]\n",
        "\n",
        "    hier_final = data_filtered[['unique_id', 'ds']]\n",
        "    # Fill NA values for these columns\n",
        "    for col in columns_to_aggregate:\n",
        "        data_quantile = data_filtered[['unique_id', 'ds','top_level', 'level2', 'level3', 'bottom_level']+[col]]\n",
        "        data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
        "        data_quantile['y'] = data_quantile['y'].fillna(0)\n",
        "\n",
        "        hier, S_df, tags = aggregate(df=data_quantile, spec=hierarchy_levels)\n",
        "\n",
        "        hier = hier.reset_index()\n",
        "        hier.rename(columns={'y': col}, inplace=True)\n",
        "\n",
        "        hier_final = hier_final.merge(hier, on = ['unique_id', 'ds'], how = 'right')\n",
        "    # Split the data into train and test sets\n",
        "    test = hier_final.groupby('unique_id').tail(fct_periods)\n",
        "    train = hier_final.drop(test.index)\n",
        "\n",
        "    return train, test, S_df, tags\n",
        "\n",
        "# Create hierarchies for forecast, actuals and budget\n",
        "rev_fct = split_unique_id_into_columns(rev_at[rev_at['ds']>=fct_st_date][['unique_id', 'ds', 'SelectedFCT']], 'unique_id')\n",
        "rev_act = split_unique_id_into_columns(rev_at[['unique_id', 'ds', 'Actuals']], 'unique_id')\n",
        "rev_bud = split_unique_id_into_columns(rev_at[rev_at['ds']>=fct_st_date][['unique_id', 'ds', 'Budget']], 'unique_id')\n",
        "\n",
        "revf_train, revf_test, S_df, tags = create_hier(rev_fct, hierarchy_levels, tested_ts, fct_periods)\n",
        "reva_train, reva_test, S_df, tags = create_hier(rev_act, hierarchy_levels, tested_ts, fct_periods)\n",
        "bud_train, bud_test, S_df, tags = create_hier(rev_bud, hierarchy_levels, tested_ts, fct_periods)"
      ],
      "metadata": {
        "id": "Wcre0Pj3HjM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reva_test['unique_id'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRVzTLzbeQpL",
        "outputId": "88160fb2-6172-45b9-c953-f269dbc5a12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "815"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# HIERARCHICAL RECONCILIATION FOR POINT FORECAST\n",
        "########################\n",
        "# Select reconcilers\n",
        "reconcilers = [\n",
        "    TopDown(method='forecast_proportions')\n",
        "    # OptimalCombination(method = 'ols', nonnegative=True)\n",
        "    # BottomUp()\n",
        "    # ERM(method='closed')\n",
        "]\n",
        "\n",
        "# Rename y to model name\n",
        "revf_test.rename(columns={'y': 'model'}, inplace=True)\n",
        "\n",
        "# Reconcile the base predictions\n",
        "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
        "revf_rec = hrec.reconcile(Y_hat_df=revf_test.set_index('unique_id'), Y_df=reva_train.set_index('unique_id'),\n",
        "                          S=S_df, tags=tags)\n",
        "\n",
        "# Reset Index and columns\n",
        "revf_rec = revf_rec[['ds', revf_rec.columns[2]]]\n",
        "revf_rec = revf_rec.reset_index()\n",
        "revf_rec.columns = ['unique_id', 'ds', 'Forecast_H']"
      ],
      "metadata": {
        "id": "N4iMkTjqI315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "c0d7f606-bf5a-4e4e-e6d3-47b55e574827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 7338 into shape (815,newaxis)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-832d0e4406a6>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Reconcile the base predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mhrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHierarchicalReconciliation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconcilers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreconcilers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m revf_rec = hrec.reconcile(Y_hat_df=revf_test.set_index('unique_id'), Y_df=reva_train.set_index('unique_id'),\n\u001b[0m\u001b[1;32m     18\u001b[0m                           S=S_df, tags=tags)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hierarchicalforecast/core.py\u001b[0m in \u001b[0;36mreconcile\u001b[0;34m(self, Y_hat_df, S, tags, Y_df, level, intervals_method, num_samples, seed, sort_df, is_balanced)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mrecmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{model_name}/{reconcile_fn_name}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_hat_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                 \u001b[0mreconciler_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_hat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7338 into shape (815,newaxis)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# HIERARCHICAL RECONCILIATION FOR PROBABILISTIC FORECAST\n",
        "########################\n",
        "quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995] # Define your quantiles\n",
        "weights = {0.005: 1, 0.025: 1, 0.165: 1, 0.250: 1, 0.500: 1, 0.750: 1, 0.835: 1, 0.975: 1, 0.995: 1}\n",
        "\n",
        "# Get ranges\n",
        "Y_hier_df, S_df, tags = aggregate(df=volume_act, spec=hierarchy_levels)\n",
        "Y_hier_df = Y_hier_df.reset_index()\n",
        "\n",
        "#split train/test sets\n",
        "Y_test_df  = Y_hier_df.groupby('unique_id').tail(fct_periods)\n",
        "Y_train_df = Y_hier_df.drop(Y_test_df.index)\n",
        "\n",
        "# Compute base predictions\n",
        "fcst = StatsForecast(df=Y_train_df,\n",
        "                     models=[AutoETS(season_length=12)],\n",
        "                     freq='MS', n_jobs=-1)\n",
        "\n",
        "Y_hat_df = fcst.forecast(h=fct_periods, fitted=True, level=quantiles)\n",
        "\n",
        "Y_fitted_df = fcst.forecast_fitted_values()\n",
        "Y_fitted_df = Y_fitted_df[['unique_id', 'ds', 'y', 'AutoETS', 'AutoETS-lo-0.995','AutoETS-lo-0.975', 'AutoETS-lo-0.835', 'AutoETS-lo-0.75', 'AutoETS-hi-0.75', 'AutoETS-hi-0.835','AutoETS-hi-0.975', 'AutoETS-hi-0.995']]\n",
        "Y_fitted_df.columns = ['unique_id', 'ds', 'y', 'model', 'model-lo-99.5','model-lo-97.5', 'model-lo-83.5', 'model-lo-75', 'model-hi-75', 'model-hi-83.5','model-hi-97.5', 'model-hi-99.5']\n",
        "\n",
        "# Create probabilistic dataframe\n",
        "arima_df.rename(columns={'ARIMA': 'y'}, inplace=True)\n",
        "ets_df.rename(columns={'ETS': 'y'}, inplace=True)\n",
        "\n",
        "arima_dfp = arima_df[arima_df['unique_id'].isin(arima_ids)]\n",
        "ets_dfp = ets_df[ets_df['unique_id'].isin(ets_ids)]\n",
        "\n",
        "rev_prb = pd.concat([arima_dfp, ets_dfp])\n",
        "\n",
        "# Split 'unique_id' into 4 new columns\n",
        "split_columns = rev_prb['unique_id'].str.split('/', expand=True)\n",
        "\n",
        "# Rename the columns\n",
        "split_columns.columns = ['top_level', 'level2', 'level3', 'bottom_level']\n",
        "rev_prb = pd.concat([rev_prb, split_columns], axis=1)\n",
        "\n",
        "# Filter\n",
        "rev_prb = rev_prb[rev_prb['unique_id'].isin(tested_ts)]\n",
        "\n",
        "rev_prb_train, rev_prb_test, S_df, tags = create_hier(rev_prb, hierarchy_levels, tested_ts, fct_periods)\n",
        "\n",
        "\n",
        "reconcilers = [\n",
        "    BottomUp(),\n",
        "# TopDown(method='forecast_proportions')\n",
        "]\n",
        "\n",
        "# Rename y to model name\n",
        "rev_prb_test.columns = ['unique_id', 'ds', 'model-lo-99.5','model-lo-97.5', 'model-lo-83.5', 'model-lo-75', 'model', 'model-hi-75', 'model-hi-83.5','model-hi-97.5', 'model-hi-99.5']\n",
        "\n",
        "Y_fitted_df = Y_fitted_df[Y_fitted_df['unique_id'].isin(rev_prb_test['unique_id'])]\n",
        "\n",
        "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
        "rev_prb_rec = hrec.reconcile(Y_hat_df=rev_prb_test.set_index('unique_id'), Y_df=Y_fitted_df.set_index('unique_id'),\n",
        "                          S=S_df, tags=tags,level=[75, 83.5, 97.5, 99.5], intervals_method='normality')\n",
        "\n",
        "rev_prb_rec.rename(columns={'model': 'Forecast'}, inplace=True)\n",
        "\n",
        "rev_prb_rec = rev_prb_rec.reset_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P6M_3GqR7tk",
        "outputId": "16476c77-53c2-4b56-a4ca-e1e392fb1069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:399: FutureWarning: The `df` argument of the StatsForecast constructor as well as reusing stored dfs from other methods is deprecated and will raise an error in a future version. Please provide the `df` argument to the corresponding method instead, e.g. fit/forecast.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/statsforecast/core.py:399: FutureWarning: The `df` argument of the StatsForecast constructor as well as reusing stored dfs from other methods is deprecated and will raise an error in a future version. Please provide the `df` argument to the corresponding method instead, e.g. fit/forecast.\n",
            "  warnings.warn(\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n",
            "<ipython-input-17-a32677b15a15>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile.rename(columns={col: 'y'}, inplace=True)\n",
            "<ipython-input-17-a32677b15a15>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_quantile['y'] = data_quantile['y'].fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(Y_fitted_df['unique_id'].unique()).difference(set(rev_prb_test['unique_id'].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEFTLuTsq7t0",
        "outputId": "5f3412dc-f411-45be-9534-0ce2e2d279c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'global/ENZA - Enzalutamide/',\n",
              " 'global/ENZA - Enzalutamide//D_I_MEA_HQ - MEA HQ',\n",
              " 'global/MIRABEGRON - Mirabegron/',\n",
              " 'global/MIRABEGRON - Mirabegron//D_E_MSM_HUB - Mid Sized Markets Area Hub'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# CREATE DATAFRAME TO PLOT\n",
        "########################\n",
        "bud_test.rename(columns={'y': 'Budget'}, inplace=True)\n",
        "\n",
        "# Update Actuals columns\n",
        "reva = pd.concat([reva_train, reva_test])\n",
        "reva['Actuals'] = reva.apply(lambda row: row['y'] if row['ds'] >= fct_st_date else None, axis=1)\n",
        "reva['Actuals (Train)'] = reva.apply(lambda row: row['y'] if row['ds'] < fct_st_date else None, axis=1)\n",
        "\n",
        "# Update forecast\n",
        "rev_prb_rec = rev_prb_rec[rev_prb_rec['ds']>=fct_st_date]\n",
        "\n",
        "# Merge\n",
        "rev_at_hier = reva.merge(rev_prb_rec[['unique_id', 'ds', 'Forecast']], on=['unique_id', 'ds'], how='left')\n",
        "rev_at_hier = rev_at_hier.merge(bud_test, on=['unique_id', 'ds'], how='left')\n",
        "rev_at_hier2plot = rev_at_hier.drop(columns=['y'])\n",
        "\n",
        "rev_prb_rec = rev_prb_rec[['unique_id', 'ds', 'model-lo-99.5', 'model-lo-97.5', 'model-lo-83.5',\n",
        "       'model-lo-75', 'Forecast', 'model-hi-75', 'model-hi-83.5',\n",
        "       'model-hi-97.5', 'model-hi-99.5']]"
      ],
      "metadata": {
        "id": "p38A2zrxJK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# METRICS\n",
        "########################\n",
        "\n",
        "def sum_of_differences(time_series):\n",
        "    # Find the index of the first non-zero value\n",
        "    first_non_zero_index = next((index for index, value in enumerate(time_series) if value != 0), None)\n",
        "\n",
        "    # Check if there is a non-zero value in the series\n",
        "    if first_non_zero_index is None:\n",
        "        return 0  # Return 0 if there are no non-zero values\n",
        "\n",
        "    # Calculate the sum of the differences after the first non-zero value\n",
        "    sum_diff = sum(abs(time_series[i] - time_series[i - 1]) for i in range(first_non_zero_index + 1, len(time_series)))\n",
        "\n",
        "    # Calculate the number of time points after the first non-zero value minus one\n",
        "    num_points = len(time_series) - first_non_zero_index - 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if num_points <= 0:\n",
        "        return 0\n",
        "\n",
        "    # Return the result\n",
        "    return np.array((sum_diff / num_points).values())[0][0]\n",
        "\n",
        "\n",
        "def metrics(actual_data, forecasted_data, quantiles, weights, sample_columns, stochastic=False):\n",
        "    # Prepare a list to store WSPL results for each unique_id\n",
        "    results = []\n",
        "\n",
        "    # Ensure 'ds' is in datetime format\n",
        "    actual_data['ds'] = pd.to_datetime(actual_data['ds'])\n",
        "\n",
        "    for unique_id in actual_data['unique_id'].unique():\n",
        "        try:\n",
        "            wspl, rmse_metric, rmsse_metric, ope_metric = np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "            # Filter the actual data\n",
        "            actual_values = actual_data[(actual_data['unique_id'] == unique_id) & (actual_data['ds'] >= fct_st_date)][['ds', 'y']].tail(fct_periods)\n",
        "\n",
        "            actual_ts = TimeSeries.from_dataframe(actual_values.set_index('ds'))\n",
        "\n",
        "            historical_actuals = actual_data[actual_data['unique_id'] == unique_id][['ds', 'y']].drop(actual_values.index)\n",
        "\n",
        "            historical_ts = TimeSeries.from_dataframe(historical_actuals.set_index('ds'))\n",
        "\n",
        "            # Filter the forecasted data\n",
        "            forecasted_values = forecasted_data[forecasted_data['unique_id']==unique_id]\n",
        "\n",
        "            forecasted_values = forecasted_values.sort_values('ds')\n",
        "\n",
        "            # Find the unique time points\n",
        "            unique_times = forecasted_values['ds'].unique()\n",
        "            num_times = len(unique_times)\n",
        "\n",
        "            # Define the number of components and samples\n",
        "            num_components = 1  # 'y'\n",
        "            num_samples = len(sample_columns)   # Number of forecast columns\n",
        "\n",
        "            # Initialize the 3D array\n",
        "            array_3d = np.zeros((num_times, num_components, num_samples))\n",
        "\n",
        "            # Fill in the array\n",
        "            for i, time in enumerate(unique_times):\n",
        "                # Select the corresponding rows from the DataFrame\n",
        "                row = forecasted_values[forecasted_values['ds'] == time]\n",
        "                # Extract the sample values and assign them to the array\n",
        "                samples = row[sample_columns].to_numpy().reshape(num_samples)\n",
        "                array_3d[i, 0, :] = samples\n",
        "\n",
        "            # Convert the 'ds' column to datetime\n",
        "            forecasted_values['ds'] = pd.to_datetime(forecasted_values['ds'])\n",
        "\n",
        "            # Create a DatetimeIndex from the 'ds' column\n",
        "            datetime_index = pd.DatetimeIndex(forecasted_values['ds'])\n",
        "\n",
        "            forecasted_ts = TimeSeries.from_times_and_values(datetime_index, array_3d)\n",
        "\n",
        "            # Initialize losses dictionary\n",
        "            losses = {}\n",
        "\n",
        "            if stochastic:\n",
        "                # Calculate quantile loss for each quantile if stochastic is True\n",
        "                for q in quantiles:\n",
        "                    try:\n",
        "                        losses[q] = quantile_loss(actual_ts, forecasted_ts, q)\n",
        "                    except Exception as e:  # Use appropriate exception handling based on your quantile_loss function\n",
        "                        print(f\"Error calculating quantile loss for {q}: {e}\")\n",
        "                        losses[q] = np.nan\n",
        "            else:\n",
        "                # Set all losses to NaN if stochastic is False\n",
        "                for q in quantiles:\n",
        "                    losses[q] = np.nan\n",
        "\n",
        "            wspl = sum(weights[q] * losses.get(q, np.nan) for q in quantiles) / sum(weights.values()) if stochastic else np.nan\n",
        "\n",
        "            # Calculate RMSE and RMSSE\n",
        "            mse_metric = mse(actual_ts, forecasted_ts)\n",
        "            scaled = sum_of_differences(historical_ts)\n",
        "\n",
        "            rmse_metric = np.sqrt(mse_metric)\n",
        "            rmsse_metric = np.sqrt((mse_metric / scaled)) if scaled != 0 else np.nan\n",
        "\n",
        "            # Calculate the overall percentage error\n",
        "            try:\n",
        "                ope_metric = ope(actual_ts, forecasted_ts)\n",
        "            except Exception as e:\n",
        "                ope_metric = np.nan\n",
        "\n",
        "            # Append the result to the list\n",
        "            results.append({'unique_id': unique_id, 'WSPL': wspl, 'RMSE': rmse_metric, 'RMSSE': rmsse_metric, 'OPE': ope_metric})\n",
        "        except Exception as e:\n",
        "            # If an error occurs, log it, and append NaN metrics for this unique_id\n",
        "            print(f\"An error occurred for {unique_id}: {e}\")\n",
        "            results.append({'unique_id': unique_id, 'WSPL': np.nan, 'RMSE': np.nan, 'RMSSE': np.nan, 'OPE': np.nan})\n",
        "\n",
        "    # Convert the list of results into a DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# Usage:\n",
        "quantiles = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995] # Define your quantiles\n",
        "weights = {0.005: 1, 0.025: 1, 0.165: 1, 0.250: 1, 0.500: 1, 0.750: 1, 0.835: 1, 0.975: 1, 0.995: 1}\n",
        "sample_columns_fct = ['model-lo-99.5', 'model-lo-97.5', 'model-lo-83.5', 'model-lo-75','Forecast', 'model-hi-75', 'model-hi-83.5', 'model-hi-97.5','model-hi-99.5']\n",
        "\n",
        "\n",
        "rev_at_hier = rev_at_hier[rev_at_hier['unique_id'].isin(rev_prb_rec['unique_id'])]\n",
        "\n",
        "# rev_prb_rec_q = rev_prb_rec[rev_prb_rec['unique_id']=='global/ENZA - Enzalutamide/']\n",
        "# rev_at_hier_q = rev_at_hier[rev_at_hier['unique_id']=='global/ENZA - Enzalutamide/']\n",
        "fct_results = metrics(rev_at_hier[['unique_id', 'ds', 'y']], rev_prb_rec, quantiles, weights, sample_columns_fct, stochastic=True)\n",
        "bud_results = metrics(rev_at_hier[['unique_id', 'ds', 'y']], bud_test, quantiles, weights, ['Budget'], stochastic=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHRgSwkvceoG",
        "outputId": "ddd9a335-5f80-42ff-af25-403b2e477d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-c4dd733498dd>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  actual_data['ds'] = pd.to_datetime(actual_data['ds'])\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.timeseries:ValueError: The time index of the provided DataArray is missing the freq attribute, and the frequency could not be directly inferred. This probably comes from inconsistent date frequencies with missing dates. If you know the actual frequency, try setting `fill_missing_dates=True, freq=actual_frequency`. If not, try setting `fill_missing_dates=True, freq=None` to see if a frequency can be inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred for global/SOLIFENACN - Solifenacin Succinate/Japan/JP10 - Astellas Pharma Inc: The time index of the provided DataArray is missing the freq attribute, and the frequency could not be directly inferred. This probably comes from inconsistent date frequencies with missing dates. If you know the actual frequency, try setting `fill_missing_dates=True, freq=actual_frequency`. If not, try setting `fill_missing_dates=True, freq=None` to see if a frequency can be inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "<ipython-input-58-c4dd733498dd>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  actual_data['ds'] = pd.to_datetime(actual_data['ds'])\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.timeseries:ValueError: The time index of the provided DataArray is missing the freq attribute, and the frequency could not be directly inferred. This probably comes from inconsistent date frequencies with missing dates. If you know the actual frequency, try setting `fill_missing_dates=True, freq=actual_frequency`. If not, try setting `fill_missing_dates=True, freq=None` to see if a frequency can be inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred for global/SOLIFENACN - Solifenacin Succinate/Japan/JP10 - Astellas Pharma Inc: The time index of the provided DataArray is missing the freq attribute, and the frequency could not be directly inferred. This probably comes from inconsistent date frequencies with missing dates. If you know the actual frequency, try setting `fill_missing_dates=True, freq=actual_frequency`. If not, try setting `fill_missing_dates=True, freq=None` to see if a frequency can be inferred.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n",
            "ERROR:darts.metrics.metrics:ValueError: The series of actual value cannot sum to zero when computing OPE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# RESULTS DATAFRAME\n",
        "########################\n",
        "# Ensure 'ds' is in datetime format\n",
        "rev_at_hier['ds'] = pd.to_datetime(rev_at_hier['ds'])\n",
        "\n",
        "# Filter the DataFrame for dates within the specified range\n",
        "results = rev_at_hier[(rev_at_hier['ds'] >= fct_st_date) & (rev_at_hier['ds'] <= fct_end_date)]\n",
        "results = results.groupby('unique_id')[['Actuals', 'Budget', 'Forecast']].sum().reset_index()\n",
        "\n",
        "def create_output(results, fct_results, metrics, col_nme):\n",
        "    # Merge results with forecast results\n",
        "    results_fct = results[['unique_id', 'Actuals', col_nme]].merge(fct_results, how='left', on='unique_id')\n",
        "\n",
        "    # Split 'unique_id' into separate levels\n",
        "    split_columns = results_fct['unique_id'].str.split('/', expand=True)\n",
        "    split_columns.columns = ['Global', 'Product', 'Region', 'Country']\n",
        "\n",
        "    # Concatenate split columns back to the original dataframe\n",
        "    results_fct = pd.concat([results_fct, split_columns], axis=1)\n",
        "\n",
        "    # Rearrange and rename columns\n",
        "    results_fct = results_fct[['unique_id', 'Global', 'Product', 'Region', 'Country', 'Actuals', col_nme, 'WSPL', 'RMSE', 'RMSSE', 'OPE']]\n",
        "    results_fct.columns = ['unique_id', 'Global', 'Product', 'Region', 'Country', 'Actuals', col_nme, 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "\n",
        "    # Fill missing values with placeholders\n",
        "    results_fct['Product'] = results_fct['Product'].fillna('')\n",
        "    results_fct['Region'] = results_fct['Region'].fillna('')\n",
        "    results_fct['Country'] = results_fct['Country'].fillna('')\n",
        "\n",
        "    # Filter for Product Level where 'Region' is empty\n",
        "    results_fct_prod = results_fct[results_fct['Region'] == ''][['Global', 'Product', 'Actuals', col_nme, 'RMSE', 'RMSSE', 'Error %', 'SPL']]\n",
        "\n",
        "    # Initialize a dictionary to hold the pivoted DataFrames for each metric\n",
        "    pivot_dfs = {}\n",
        "\n",
        "    for metric in metrics:\n",
        "        pivot_df = results_fct.pivot_table(\n",
        "            index=['Global', 'Product'],\n",
        "            columns=['Region', 'Country'],\n",
        "            values=metric,\n",
        "            aggfunc='sum'  # Change as needed\n",
        "        ).reset_index()\n",
        "\n",
        "        # Add the pivoted DataFrame to the dictionary\n",
        "        pivot_dfs[metric] = pivot_df\n",
        "\n",
        "    return pivot_dfs, results_fct_prod\n",
        "\n",
        "# Forecast\n",
        "metrics = ['Actuals', 'Forecast', 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "pivot_dfs, results_fct_prod = create_output(results, fct_results, metrics, 'Forecast')\n",
        "actuals_df = pivot_dfs['Actuals']\n",
        "output_df = pivot_dfs['Forecast']\n",
        "rmse_df = pivot_dfs['RMSE']\n",
        "rmsse_df = pivot_dfs['RMSSE']\n",
        "ope_df = pivot_dfs['Error %']\n",
        "spl_df = pivot_dfs['SPL']\n",
        "\n",
        "# Budget\n",
        "metrics = ['Actuals', 'Budget', 'SPL', 'RMSE', 'RMSSE', 'Error %']\n",
        "pivot_dfs2, results_bud_prod = create_output(results, bud_results, metrics, 'Budget')\n",
        "actuals_df2 = pivot_dfs2['Actuals']\n",
        "output_df2 = pivot_dfs2['Budget']\n",
        "rmse_df2 = pivot_dfs2['RMSE']\n",
        "rmsse_df2 = pivot_dfs2['RMSSE']\n",
        "ope_df2 = pivot_dfs2['Error %']\n",
        "spl_df2 = pivot_dfs2['SPL']\n",
        "\n",
        "# Get column starts\n",
        "col_starts = [1, results_fct_prod.shape[1]+1, output_df.shape[1]+2, rmse_df.shape[1]+2, rmsse_df.shape[1]+2, ope_df.shape[1]+2]\n",
        "col_starts_sum = []\n",
        "running_total = 0\n",
        "\n",
        "for value in col_starts:\n",
        "    running_total += value\n",
        "    col_starts_sum.append(running_total)\n",
        "\n",
        "\n",
        "# Create a Pandas Excel writer using openpyxl as the engine\n",
        "filename='/content/drive/MyDrive/Colab Notebooks/Revenue Prediction/data/consolidated_results.xlsx'\n",
        "with pd.ExcelWriter(filename) as writer:\n",
        "    results_fct_prod.to_excel(writer, sheet_name='Sheet1', startrow=3, startcol=col_starts_sum[0], header=True, index=False)\n",
        "    output_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[1], header=True)\n",
        "    rmse_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[2], header=True)\n",
        "    rmsse_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[3], header=True)\n",
        "    ope_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[4], header=True)\n",
        "    spl_df.to_excel(writer, sheet_name='Sheet1', startrow=2, startcol=col_starts_sum[5], header=True)\n",
        "\n",
        "    results_bud_prod.to_excel(writer, sheet_name='Sheet2', startrow=3, startcol=col_starts_sum[0], header=True, index=False)\n",
        "    output_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[1], header=True)\n",
        "    rmse_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[2], header=True)\n",
        "    rmsse_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[3], header=True)\n",
        "    ope_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[4], header=True)\n",
        "    spl_df2.to_excel(writer, sheet_name='Sheet2', startrow=2, startcol=col_starts_sum[5], header=True)"
      ],
      "metadata": {
        "id": "tA8yuf2E3AEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "# PLOT\n",
        "########################\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Update the function to include filtering based on 'unique_id'\n",
        "def plot_data(unique_id):\n",
        "    # Define x_column and y_columns directly\n",
        "    x_column = data2use.columns[1]\n",
        "    y_columns = [data2use.columns[2], data2use.columns[3], data2use.columns[4], data2use.columns[5]]\n",
        "\n",
        "    # Filter data based on selected unique_id\n",
        "    filtered_data = data2use[data2use['unique_id'] == unique_id]\n",
        "\n",
        "    # Set up a 1x3 grid of subplots\n",
        "    fig, (ax1, ax4) = plt.subplots(1, 2, figsize=(25, 5), gridspec_kw={'width_ratios': [4, 1]}) # Adjust layout for table\n",
        "\n",
        "    # Plotting multiple y-axes on the first subplot\n",
        "    for y_column in y_columns:\n",
        "        ax1.plot(filtered_data[x_column], filtered_data[y_column], label=y_column)\n",
        "    ax1.set_xlabel(x_column)\n",
        "    ax1.set_ylabel('Values')\n",
        "    ax1.set_title(f'Revenue for {unique_id}')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Remove axis for table\n",
        "    ax4.axis('off')\n",
        "    ax4.axis('tight')\n",
        "\n",
        "    # Displaying the sum table\n",
        "    display_data = filtered_data[[x_column] + list(y_columns)].copy()\n",
        "    display_data = display_data[display_data['ds']>=fct_st_date]\n",
        "    display_data['ds'] = display_data['ds'].dt.strftime('%m/%d/%Y')\n",
        "\n",
        "    # Create a sum row\n",
        "    sum_values = {x_column: 'Sum'}\n",
        "    for col in list(y_columns):\n",
        "        sum_values[col] = display_data[col].sum()\n",
        "    sum_row = pd.DataFrame([sum_values])\n",
        "\n",
        "    # Create a % diff row\n",
        "    actuals_sum = sum_values['Actuals']\n",
        "    pdiff_values = {x_column: '% Diff'}\n",
        "    for col in list(y_columns):\n",
        "        pdiff_values[col] = ((display_data[col].sum()-actuals_sum) / actuals_sum) * 100 if actuals_sum != 0 else None\n",
        "        pdiff_values[col] = round(pdiff_values[col], 2)\n",
        "    perc_diff_row = pd.DataFrame([pdiff_values])\n",
        "\n",
        "    # Stack the sum row\n",
        "    display_data = pd.concat([sum_row, display_data], ignore_index=True)\n",
        "\n",
        "    # Round the values and add commas\n",
        "    for column in y_columns:\n",
        "        if column in display_data.columns:\n",
        "            # Round to two decimal places\n",
        "            display_data[column] = display_data[column].round(2)\n",
        "            # Format with commas\n",
        "            display_data[column] = display_data[column].apply(lambda x: f\"{x:,.2f}\")\n",
        "\n",
        "    # Stack the % diff and remove 'Actuals Train'\n",
        "    display_data = pd.concat([perc_diff_row, display_data], ignore_index=True)\n",
        "    display_data = display_data.drop('Actuals (Train)', axis=1)\n",
        "\n",
        "    # Convert perc_diff_data to array for table\n",
        "    table_data = display_data.to_numpy()\n",
        "    # Add table at the right\n",
        "    table = ax4.table(cellText=table_data, colLabels=display_data.columns, loc='right')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(8.5)  # Set smaller font size if necessary\n",
        "    table.scale(4, 1.8)  # Adjust scale to fit\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# data2use = ts2fix\n",
        "# data2use = tsnonspend\n",
        "# data2use = data2plot\n",
        "data2use = rev_at_hier2plot\n",
        "\n",
        "# Create widgets\n",
        "unique_id_selector = widgets.SelectionSlider(\n",
        "    options=data2use['unique_id'].unique(),\n",
        "    description='unique_id:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "# Display interactive plot\n",
        "interact(plot_data, unique_id=unique_id_selector)"
      ],
      "metadata": {
        "id": "orhsNKOrfL7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XthKTDgx29ol"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i1lK0A7UPSkqEp7I06G70oHrSC0UDtQy",
      "authorship_tag": "ABX9TyNuywCX8EFI89GPb4rHbIsM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}